{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ECSE552_Project.ipynb","provenance":[{"file_id":"13Q0F-QriPz63riV8GkSHXs5EuAfzDjaa","timestamp":1614612026872}],"collapsed_sections":["rfFv_cyRvyRC"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oZ1PQn6JMpbU"},"source":["# Load Library/Dependency Installations from Google Drive\n","\n","If you have installed ibm aihwkit."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlYoU_FUdxal","executionInfo":{"status":"ok","timestamp":1618851494174,"user_tz":-180,"elapsed":5241,"user":{"displayName":"Amna Sajjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM06eCwjqruN0wUfda-PTPlTbHTk3jGwGxRrY3Pg=s64","userId":"03051534757007455985"}},"outputId":"24ee8e09-16cf-485c-a9c3-707b1b3792c1"},"source":["import torch\n","\n","# Check device\n","USE_CUDA = 0\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n"," USE_CUDA = 1\n"," print(f\"Nvidia Cuda/GPU is available!\")\n","else:\n","  print (\"not available\") \n"," \n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n"," print(gpu_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Nvidia Cuda/GPU is available!\n","Mon Apr 19 16:58:13 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSPjqFbO7gAU","executionInfo":{"status":"ok","timestamp":1618851576878,"user_tz":-180,"elapsed":80688,"user":{"displayName":"Amna Sajjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM06eCwjqruN0wUfda-PTPlTbHTk3jGwGxRrY3Pg=s64","userId":"03051534757007455985"}},"outputId":"3fb54b4d-4248-454b-e8fc-b9cbb5f44a68"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive' )\n","\n","# note change to ur own path\n","# %cd '/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final'\n","# %cd '/content/gdrive/MyDrive/ECSE552final'\n","%cd '/content/gdrive/MyDrive/aihw_venv'\n","%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/aihw_venv\n","\u001b[0m\u001b[01;34maihwkit_env\u001b[0m/  \u001b[01;34mdata\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rfFv_cyRvyRC"},"source":["# NOTE!!!!! DON't run these lines if you already install IBM library!! \n","# Install library\n","If you haven't installed ibm aihwkit, please run following blocks.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf17OULJs8om","executionInfo":{"elapsed":1313,"status":"ok","timestamp":1618492915865,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"298bd8ff-041a-40b6-98e1-cb151935ad2f"},"source":["# \"\"\"Good to run this just incase you are in the wrong directory\"\"\"\n","# %cd '/content/gdrive/MyDrive/aihw_venv'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/aihw_venv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x54htSB2CqJb","executionInfo":{"elapsed":8710,"status":"ok","timestamp":1618488279067,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"11450a40-5189-4d6c-84ab-2f565686c6a2"},"source":["# # Creating python virtual env for aihwkit\n","# %%script bash\n","# apt-get install python3-venv\n","# python3 -m venv aihwkit_env\n","# cd aihwkit_env/\n","# source bin/activate\n","# git clone https://github.com/IBM/aihwkit.git\n","# cd aihwkit\n","# ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","python3-venv is already the newest version (3.6.7-1~18.04).\n","0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n","CHANGELOG.md\n","cmake\n","CMakeLists.txt\n","docs\n","examples\n","LICENSE.txt\n","Makefile\n","MANIFEST.in\n","README.md\n","requirements-dev.txt\n","requirements-examples.txt\n","requirements.txt\n","setup.cfg\n","setup.py\n","src\n","tests\n"],"name":"stdout"},{"output_type":"stream","text":["Error: Command '['/content/gdrive/MyDrive/aihw_venv/aihwkit_env/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n","bash: line 4: bin/activate: No such file or directory\n","Cloning into 'aihwkit'...\n","Checking out files:  29% (100/337)   \rChecking out files:  30% (102/337)   \rChecking out files:  31% (105/337)   \rChecking out files:  32% (108/337)   \rChecking out files:  33% (112/337)   \rChecking out files:  34% (115/337)   \rChecking out files:  35% (118/337)   \rChecking out files:  36% (122/337)   \rChecking out files:  37% (125/337)   \rChecking out files:  38% (129/337)   \rChecking out files:  39% (132/337)   \rChecking out files:  40% (135/337)   \rChecking out files:  41% (139/337)   \rChecking out files:  42% (142/337)   \rChecking out files:  43% (145/337)   \rChecking out files:  44% (149/337)   \rChecking out files:  45% (152/337)   \rChecking out files:  46% (156/337)   \rChecking out files:  47% (159/337)   \rChecking out files:  48% (162/337)   \rChecking out files:  49% (166/337)   \rChecking out files:  50% (169/337)   \rChecking out files:  51% (172/337)   \rChecking out files:  52% (176/337)   \rChecking out files:  53% (179/337)   \rChecking out files:  54% (182/337)   \rChecking out files:  55% (186/337)   \rChecking out files:  56% (189/337)   \rChecking out files:  57% (193/337)   \rChecking out files:  57% (195/337)   \rChecking out files:  58% (196/337)   \rChecking out files:  59% (199/337)   \rChecking out files:  60% (203/337)   \rChecking out files:  61% (206/337)   \rChecking out files:  62% (209/337)   \rChecking out files:  63% (213/337)   \rChecking out files:  64% (216/337)   \rChecking out files:  65% (220/337)   \rChecking out files:  66% (223/337)   \rChecking out files:  67% (226/337)   \rChecking out files:  68% (230/337)   \rChecking out files:  69% (233/337)   \rChecking out files:  70% (236/337)   \rChecking out files:  71% (240/337)   \rChecking out files:  72% (243/337)   \rChecking out files:  73% (247/337)   \rChecking out files:  74% (250/337)   \rChecking out files:  75% (253/337)   \rChecking out files:  76% (257/337)   \rChecking out files:  77% (260/337)   \rChecking out files:  78% (263/337)   \rChecking out files:  79% (267/337)   \rChecking out files:  80% (270/337)   \rChecking out files:  81% (273/337)   \rChecking out files:  82% (277/337)   \rChecking out files:  83% (280/337)   \rChecking out files:  84% (284/337)   \rChecking out files:  85% (287/337)   \rChecking out files:  86% (290/337)   \rChecking out files:  86% (291/337)   \rChecking out files:  87% (294/337)   \rChecking out files:  88% (297/337)   \rChecking out files:  89% (300/337)   \rChecking out files:  90% (304/337)   \rChecking out files:  91% (307/337)   \rChecking out files:  92% (311/337)   \rChecking out files:  93% (314/337)   \rChecking out files:  94% (317/337)   \rChecking out files:  95% (321/337)   \rChecking out files:  96% (324/337)   \rChecking out files:  97% (327/337)   \rChecking out files:  98% (331/337)   \rChecking out files:  99% (334/337)   \rChecking out files: 100% (337/337)   \rChecking out files: 100% (337/337), done.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ItgRdAxaN9MY","executionInfo":{"elapsed":12379,"status":"ok","timestamp":1618488298193,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"2a02e30e-bc21-4e75-d66c-8ed41aac5246"},"source":["# # # Installing dependencies in the virtual environment\n","# %%script bash\n","# sudo apt-get install ninja-build\n","# sudo apt-get install libopenblas-dev\n","# pip install pybind11 scikit-build\n","# pip install cmake --upgrade\n","# # pip install aihwkit \n","# pip list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","ninja-build is already the newest version (1.8.2-1).\n","0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","libopenblas-dev is already the newest version (0.2.20+ds-4).\n","0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n","Requirement already satisfied: pybind11 in /usr/local/lib/python3.7/dist-packages (2.6.2)\n","Requirement already satisfied: scikit-build in /usr/local/lib/python3.7/dist-packages (0.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scikit-build) (20.9)\n","Requirement already satisfied: setuptools>=28.0.0; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from scikit-build) (54.2.0)\n","Requirement already satisfied: distro in /usr/local/lib/python3.7/dist-packages (from scikit-build) (1.5.0)\n","Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.7/dist-packages (from scikit-build) (0.36.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scikit-build) (2.4.7)\n","Requirement already up-to-date: cmake in /usr/local/lib/python3.7/dist-packages (3.18.4.post1)\n","Package                       Version       \n","----------------------------- --------------\n","absl-py                       0.12.0        \n","aihwkit                       0.3.0         \n","alabaster                     0.7.12        \n","albumentations                0.1.12        \n","altair                        4.1.0         \n","appdirs                       1.4.4         \n","argon2-cffi                   20.1.0        \n","astor                         0.8.1         \n","astropy                       4.2.1         \n","astunparse                    1.6.3         \n","async-generator               1.10          \n","atari-py                      0.2.6         \n","atomicwrites                  1.4.0         \n","attrs                         20.3.0        \n","audioread                     2.1.9         \n","autograd                      1.3           \n","Babel                         2.9.0         \n","backcall                      0.2.0         \n","beautifulsoup4                4.6.3         \n","bleach                        3.3.0         \n","blis                          0.4.1         \n","bokeh                         2.3.1         \n","Bottleneck                    1.3.2         \n","branca                        0.4.2         \n","bs4                           0.0.1         \n","CacheControl                  0.12.6        \n","cachetools                    4.2.1         \n","catalogue                     1.0.0         \n","certifi                       2020.12.5     \n","cffi                          1.14.5        \n","chainer                       7.4.0         \n","chardet                       3.0.4         \n","click                         7.1.2         \n","cloudpickle                   1.3.0         \n","cmake                         3.18.4.post1  \n","cmdstanpy                     0.9.5         \n","colorcet                      2.0.6         \n","colorlover                    0.3.0         \n","community                     1.0.0b1       \n","contextlib2                   0.5.5         \n","convertdate                   2.3.2         \n","coverage                      3.7.1         \n","coveralls                     0.5           \n","crcmod                        1.7           \n","cufflinks                     0.17.3        \n","cupy-cuda101                  7.4.0         \n","cvxopt                        1.2.6         \n","cvxpy                         1.0.31        \n","cycler                        0.10.0        \n","cymem                         2.0.5         \n","Cython                        0.29.22       \n","daft                          0.0.4         \n","dask                          2.12.0        \n","datascience                   0.10.6        \n","debugpy                       1.0.0         \n","decorator                     4.4.2         \n","defusedxml                    0.7.1         \n","descartes                     1.1.0         \n","dill                          0.3.3         \n","distributed                   1.25.3        \n","distro                        1.5.0         \n","dlib                          19.18.0       \n","dm-tree                       0.1.5         \n","docopt                        0.6.2         \n","docutils                      0.17          \n","dopamine-rl                   1.0.5         \n","earthengine-api               0.1.260       \n","easydict                      1.9           \n","ecos                          2.0.7.post1   \n","editdistance                  0.5.3         \n","en-core-web-sm                2.2.5         \n","entrypoints                   0.3           \n","ephem                         3.7.7.1       \n","et-xmlfile                    1.0.1         \n","fa2                           0.3.5         \n","fancyimpute                   0.4.3         \n","fastai                        1.0.61        \n","fastdtw                       0.3.4         \n","fastprogress                  1.0.0         \n","fastrlock                     0.6           \n","fbprophet                     0.7.1         \n","feather-format                0.4.1         \n","filelock                      3.0.12        \n","firebase-admin                4.4.0         \n","fix-yahoo-finance             0.0.22        \n","Flask                         1.1.2         \n","flatbuffers                   1.12          \n","folium                        0.8.3         \n","future                        0.16.0        \n","gast                          0.3.3         \n","GDAL                          2.2.2         \n","gdown                         3.6.4         \n","gensim                        3.6.0         \n","geographiclib                 1.50          \n","geopy                         1.17.0        \n","gin-config                    0.4.0         \n","glob2                         0.7           \n","google                        2.0.3         \n","google-api-core               1.26.3        \n","google-api-python-client      1.12.8        \n","google-auth                   1.28.1        \n","google-auth-httplib2          0.0.4         \n","google-auth-oauthlib          0.4.4         \n","google-cloud-bigquery         1.21.0        \n","google-cloud-bigquery-storage 1.1.0         \n","google-cloud-core             1.0.3         \n","google-cloud-datastore        1.8.0         \n","google-cloud-firestore        1.7.0         \n","google-cloud-language         1.2.0         \n","google-cloud-storage          1.18.1        \n","google-cloud-translate        1.5.0         \n","google-colab                  1.0.0         \n","google-pasta                  0.2.0         \n","google-resumable-media        0.4.1         \n","googleapis-common-protos      1.53.0        \n","googledrivedownloader         0.4           \n","graphviz                      0.10.1        \n","greenlet                      1.0.0         \n","grpcio                        1.32.0        \n","gspread                       3.0.1         \n","gspread-dataframe             3.0.8         \n","gym                           0.17.3        \n","h5py                          2.10.0        \n","HeapDict                      1.0.1         \n","hijri-converter               2.1.1         \n","holidays                      0.10.5.2      \n","holoviews                     1.14.3        \n","html5lib                      1.0.1         \n","httpimport                    0.5.18        \n","httplib2                      0.17.4        \n","httplib2shim                  0.0.3         \n","humanize                      0.5.1         \n","hyperopt                      0.1.2         \n","ideep4py                      2.0.0.post3   \n","idna                          2.10          \n","imageio                       2.4.1         \n","imagesize                     1.2.0         \n","imbalanced-learn              0.4.3         \n","imblearn                      0.0           \n","imgaug                        0.2.9         \n","importlib-metadata            3.10.0        \n","importlib-resources           5.1.2         \n","imutils                       0.5.4         \n","inflect                       2.1.0         \n","iniconfig                     1.1.1         \n","intel-openmp                  2021.2.0      \n","intervaltree                  2.1.0         \n","ipykernel                     4.10.1        \n","ipython                       5.5.0         \n","ipython-genutils              0.2.0         \n","ipython-sql                   0.3.9         \n","ipywidgets                    7.6.3         \n","itsdangerous                  1.1.0         \n","jax                           0.2.12        \n","jaxlib                        0.1.65+cuda110\n","jdcal                         1.4.1         \n","jedi                          0.18.0        \n","jieba                         0.42.1        \n","Jinja2                        2.11.3        \n","joblib                        1.0.1         \n","jpeg4py                       0.1.4         \n","jsonschema                    2.6.0         \n","jupyter                       1.0.0         \n","jupyter-client                5.3.5         \n","jupyter-console               5.2.0         \n","jupyter-core                  4.7.1         \n","jupyterlab-pygments           0.1.2         \n","jupyterlab-widgets            1.0.0         \n","kaggle                        1.5.12        \n","kapre                         0.1.3.1       \n","Keras                         2.4.3         \n","Keras-Preprocessing           1.1.2         \n","keras-vis                     0.4.1         \n","kiwisolver                    1.3.1         \n","knnimpute                     0.1.0         \n","korean-lunar-calendar         0.2.1         \n","librosa                       0.8.0         \n","lightgbm                      2.2.3         \n","llvmlite                      0.34.0        \n","lmdb                          0.99          \n","LunarCalendar                 0.0.9         \n","lxml                          4.2.6         \n","Markdown                      3.3.4         \n","MarkupSafe                    1.1.1         \n","matplotlib                    3.2.2         \n","matplotlib-venn               0.11.6        \n","missingno                     0.4.2         \n","mistune                       0.8.4         \n","mizani                        0.6.0         \n","mkl                           2019.0        \n","mlxtend                       0.14.0        \n","more-itertools                8.7.0         \n","moviepy                       0.2.3.5       \n","mpmath                        1.2.1         \n","msgpack                       1.0.2         \n","multiprocess                  0.70.11.1     \n","multitasking                  0.0.9         \n","murmurhash                    1.0.5         \n","music21                       5.5.0         \n","natsort                       5.5.0         \n","nbclient                      0.5.3         \n","nbconvert                     5.6.1         \n","nbformat                      5.1.3         \n","nest-asyncio                  1.5.1         \n","networkx                      2.5.1         \n","nibabel                       3.0.2         \n","nltk                          3.2.5         \n","notebook                      5.3.1         \n","np-utils                      0.5.12.1      \n","numba                         0.51.2        \n","numexpr                       2.7.3         \n","numpy                         1.19.5        \n","nvidia-ml-py3                 7.352.0       \n","oauth2client                  4.1.3         \n","oauthlib                      3.1.0         \n","okgrade                       0.4.3         \n","opencv-contrib-python         4.1.2.30      \n","opencv-python                 4.1.2.30      \n","openpyxl                      2.5.9         \n","opt-einsum                    3.3.0         \n","osqp                          0.6.2.post0   \n","packaging                     20.9          \n","palettable                    3.3.0         \n","pandas                        1.1.5         \n","pandas-datareader             0.9.0         \n","pandas-gbq                    0.13.3        \n","pandas-profiling              1.4.1         \n","pandocfilters                 1.4.3         \n","panel                         0.11.1        \n","param                         1.10.1        \n","parso                         0.8.2         \n","pathlib                       1.0.1         \n","patsy                         0.5.1         \n","pexpect                       4.8.0         \n","pickleshare                   0.7.5         \n","Pillow                        7.1.2         \n","pip                           19.3.1        \n","pip-tools                     4.5.1         \n","plac                          1.1.3         \n","plotly                        4.4.1         \n","plotnine                      0.6.0         \n","pluggy                        0.7.1         \n","pooch                         1.3.0         \n","portpicker                    1.3.1         \n","prefetch-generator            1.0.1         \n","preshed                       3.0.5         \n","prettytable                   2.1.0         \n","progressbar2                  3.38.0        \n","prometheus-client             0.10.1        \n","promise                       2.3           \n","prompt-toolkit                1.0.18        \n","protobuf                      3.15.8        \n","psutil                        5.4.8         \n","psycopg2                      2.7.6.1       \n","ptyprocess                    0.7.0         \n","py                            1.10.0        \n","pyarrow                       3.0.0         \n","pyasn1                        0.4.8         \n","pyasn1-modules                0.2.8         \n","pybind11                      2.6.2         \n","pycocotools                   2.0.2         \n","pycparser                     2.20          \n","pyct                          0.4.8         \n","pydata-google-auth            1.1.0         \n","pydot                         1.3.0         \n","pydot-ng                      2.0.0         \n","pydotplus                     2.0.2         \n","PyDrive                       1.3.1         \n","pyemd                         0.5.1         \n","pyerfa                        1.7.2         \n","pyglet                        1.5.0         \n","Pygments                      2.6.1         \n","pygobject                     3.26.1        \n","pymc3                         3.7           \n","PyMeeus                       0.5.11        \n","pymongo                       3.11.3        \n","pymystem3                     0.2.0         \n","PyOpenGL                      3.1.5         \n","pyparsing                     2.4.7         \n","pyrsistent                    0.17.3        \n","pysndfile                     1.3.8         \n","PySocks                       1.7.1         \n","pystan                        2.19.1.1      \n","pytest                        3.6.4         \n","python-apt                    0.0.0         \n","python-chess                  0.23.11       \n","python-dateutil               2.8.1         \n","python-louvain                0.15          \n","python-slugify                4.0.1         \n","python-utils                  2.5.6         \n","pytz                          2018.9        \n","pyviz-comms                   2.0.1         \n","PyWavelets                    1.1.1         \n","PyYAML                        3.13          \n","pyzmq                         22.0.3        \n","qdldl                         0.1.5.post0   \n","qtconsole                     5.0.3         \n","QtPy                          1.9.0         \n","regex                         2019.12.20    \n","requests                      2.25.1        \n","requests-oauthlib             1.3.0         \n","resampy                       0.2.2         \n","retrying                      1.3.3         \n","rpy2                          3.4.3         \n","rsa                           4.7.2         \n","scikit-build                  0.11.1        \n","scikit-image                  0.16.2        \n","scikit-learn                  0.22.2.post1  \n","scipy                         1.4.1         \n","screen-resolution-extra       0.0.0         \n","scs                           2.1.2         \n","seaborn                       0.11.1        \n","Send2Trash                    1.5.0         \n","setuptools                    54.2.0        \n","setuptools-git                1.2           \n","Shapely                       1.7.1         \n","simplegeneric                 0.8.1         \n","six                           1.15.0        \n","sklearn                       0.0           \n","sklearn-pandas                1.8.0         \n","smart-open                    5.0.0         \n","snowballstemmer               2.1.0         \n","sortedcontainers              2.3.0         \n","SoundFile                     0.10.3.post1  \n","spacy                         2.2.4         \n","Sphinx                        1.8.5         \n","sphinxcontrib-serializinghtml 1.1.4         \n","sphinxcontrib-websupport      1.2.4         \n","SQLAlchemy                    1.4.6         \n","sqlparse                      0.4.1         \n","srsly                         1.0.5         \n","statsmodels                   0.10.2        \n","sympy                         1.7.1         \n","tables                        3.4.4         \n","tabulate                      0.8.9         \n","tblib                         1.7.0         \n","tensorboard                   2.4.1         \n","tensorboard-plugin-wit        1.8.0         \n","tensorflow                    2.4.1         \n","tensorflow-datasets           4.0.1         \n","tensorflow-estimator          2.4.0         \n","tensorflow-gcs-config         2.4.0         \n","tensorflow-hub                0.11.0        \n","tensorflow-metadata           0.29.0        \n","tensorflow-probability        0.12.1        \n","termcolor                     1.1.0         \n","terminado                     0.9.4         \n","testpath                      0.4.4         \n","text-unidecode                1.3           \n","textblob                      0.15.3        \n","textgenrnn                    1.4.1         \n","Theano                        1.0.5         \n","thinc                         7.4.0         \n","tifffile                      2021.4.8      \n","toml                          0.10.2        \n","toolz                         0.11.1        \n","torch                         1.8.1+cu101   \n","torchsummary                  1.5.1         \n","torchtext                     0.9.1         \n","torchvision                   0.9.1+cu101   \n","tornado                       5.1.1         \n","tqdm                          4.41.1        \n","traitlets                     5.0.5         \n","tweepy                        3.10.0        \n","typeguard                     2.7.1         \n","typing-extensions             3.7.4.3       \n","tzlocal                       1.5.1         \n","uritemplate                   3.0.1         \n","urllib3                       1.24.3        \n","vega-datasets                 0.9.0         \n","wasabi                        0.8.2         \n","wcwidth                       0.2.5         \n","webencodings                  0.5.1         \n","Werkzeug                      1.0.1         \n","wheel                         0.36.2        \n","widgetsnbextension            3.5.1         \n","wordcloud                     1.5.0         \n","wrapt                         1.12.1        \n","xarray                        0.15.1        \n","xgboost                       0.90          \n","xkit                          0.0.0         \n","xlrd                          1.1.0         \n","xlwt                          1.3.0         \n","yellowbrick                   0.9.1         \n","zict                          2.0.0         \n","zipp                          3.4.1         \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2XSEk6VGkXe","executionInfo":{"elapsed":785403,"status":"ok","timestamp":1618489089145,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"1df20a9a-0945-473b-ed9f-ed383218a7f6"},"source":["# # # Building the library\n","# %%script bash\n","# cd aihwkit_env/aihwkit/\n","# make clean\n","# python setup.py build_ext --inplace -DUSE_CUDA=ON -DRPU_CUDA_ARCHITECTURES=\"60;70\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["python setup.py clean\n","running clean\n","rm -rf _skbuild\n","rm -f src/aihwkit/simulator/rpu_base.*.so\n","Not searching for unused variables given on the command line.\n","-- The C compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_cmake_test_compile/build\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Invoking cmake through scikit-build\n","-- The BLAS backend of choice:OpenBLAS\n","-- Found OpenBLAS libraries: /usr/lib/x86_64-linux-gnu/libopenblas.so\n","-- Found OpenBLAS include: /usr/include/x86_64-linux-gnu\n","-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.7m.so (found version \"3.7.10\") \n","-- Found PythonInterp: /usr/bin/python3 (found version \"3.7.10\") \n","-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.7m.so\n","-- Performing Test HAS_FLTO\n","-- Performing Test HAS_FLTO - Success\n","-- Found pybind11: /usr/local/lib/python3.7/dist-packages/pybind11/include (found version \"2.6.2\" )\n","-- Found Python: /usr/local/bin/python (found version \"3.7.10\") found components: Interpreter \n","-- Found Torch: /usr/local/lib/python3.7/dist-packages/torch/include;/usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include  \n","-- The CUDA compiler identification is NVIDIA 11.0.221\n","-- Detecting CUDA compiler ABI info\n","-- Detecting CUDA compiler ABI info - done\n","-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","-- Detecting CUDA compile features\n","-- Detecting CUDA compile features - done\n","-- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.0.221\") \n","-- Looking for pthread.h\n","-- Looking for pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","-- Found Threads: TRUE  \n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_skbuild/linux-x86_64-3.7/cmake-build\n","[1/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/cuda_math_util.cu.o\n","[2/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/cuda_util.cu.o\n","[3/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/io_manager.cu.o\n","[4/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/maximizer.cu.o\n","[5/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/pulsed_weight_updater.cu.o\n","[6/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda.cu.o\n","[7/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/noise_manager.cu.o\n","[8/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_difference_device.cu.o\n","[9/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_expstep_device.cu.o\n","[10/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/bit_line_maker.cu.o\n","[11/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_mixedprec_device.cu.o\n","[12/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu.o\n","[13/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_constantstep_device.cu.o\n","[14/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_pulsed.cu.o\n","[15/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_pulsed_device.cu.o\n","[16/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_linearstep_device.cu.o\n","[17/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_simple_device.cu.o\n","[18/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_transfer_device.cu.o\n","[19/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_vector_device.cu.o\n","[20/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/rpucuda_powstep_device.cu.o\n","[21/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/test_helper.cu.o\n","[22/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/weight_clipper_cuda.cu.o\n","[23/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/update_management_helper.cu.o\n","[24/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/dense_bit_line_maker.cpp.o\n","[25/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/math_util.cpp.o\n","[26/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/weight_drifter_cuda.cu.o\n","[27/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rng.cpp.o\n","[28/57] Building CUDA object CMakeFiles/RPU_GPU.dir/src/rpucuda/cuda/weight_modifier_cuda.cu.o\n","[29/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_constantstep_device.cpp.o\n","[30/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu.cpp.o\n","[31/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_difference_device.cpp.o\n","[32/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_expstep_device.cpp.o\n","[33/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_forward_backward_pass.cpp.o\n","[34/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_linearstep_device.cpp.o\n","[35/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_mixedprec_device.cpp.o\n","[36/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_mixedprec_device_base.cpp.o\n","[37/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_powstep_device.cpp.o\n","[38/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_pulsed.cpp.o\n","[39/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_pulsed_device.cpp.o\n","[40/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_pulsed_meta_parameter.cpp.o\n","[41/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_simple_device.cpp.o\n","[42/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_weight_updater.cpp.o\n","[43/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_vector_device.cpp.o\n","[44/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/rpu_transfer_device.cpp.o\n","[45/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/sparse_bit_line_maker.cpp.o\n","[46/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/weight_clipper.cpp.o\n","[47/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/weight_drifter.cpp.o\n","[48/57] Building CXX object CMakeFiles/RPU_CPU.dir/src/rpucuda/weight_modifier.cpp.o\n","[49/57] Linking CXX static library libRPU_CPU.a\n","[50/57] Building CXX object src/aihwkit/simulator/CMakeFiles/rpu_base.dir/rpu_base_src/rpu_base.cpp.o\n","[51/57] Linking CXX static library CMakeFiles/RPU_GPU.dir/cmake_device_link.o\n","[52/57] Linking CXX static library libRPU_GPU.a\n","[53/57] Building CXX object src/aihwkit/simulator/CMakeFiles/rpu_base.dir/rpu_base_src/rpu_base_devices.cpp.o\n","[54/57] Building CXX object src/aihwkit/simulator/CMakeFiles/rpu_base.dir/rpu_base_src/rpu_base_tiles.cpp.o\n","[55/57] Building CXX object src/aihwkit/simulator/CMakeFiles/rpu_base.dir/rpu_base_src/rpu_base_tiles_cuda.cpp.o\n","[56/57] Linking CXX shared module src/aihwkit/simulator/rpu_base.cpython-37m-x86_64-linux-gnu.so\n","[56/57] Install the project...\n","-- Install configuration: \"Release\"\n","-- Installing: /content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_skbuild/linux-x86_64-3.7/cmake-install/src/aihwkit/simulator/rpu_base.cpython-37m-x86_64-linux-gnu.so\n","-- Set runtime path of \"/content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_skbuild/linux-x86_64-3.7/cmake-install/src/aihwkit/simulator/rpu_base.cpython-37m-x86_64-linux-gnu.so\" to \"$ORIGIN\"\n","\n","\n","--------------------------------------------------------------------------------\n","-- Trying \"Ninja\" generator\n","--------------------------------\n","---------------------------\n","----------------------\n","-----------------\n","------------\n","-------\n","--\n","--\n","-------\n","------------\n","-----------------\n","----------------------\n","---------------------------\n","--------------------------------\n","-- Trying \"Ninja\" generator - success\n","--------------------------------------------------------------------------------\n","\n","Configuring Project\n","  Working directory:\n","    /content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_skbuild/linux-x86_64-3.7/cmake-build\n","  Command:\n","    cmake '/content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit' -G Ninja '-DCMAKE_INSTALL_PREFIX:PATH=/content/gdrive/My Drive/aihw_venv/aihwkit_env/aihwkit/_skbuild/linux-x86_64-3.7/cmake-install' -DPYTHON_EXECUTABLE:FILEPATH=/usr/bin/python3 -DPYTHON_VERSION_STRING:STRING=3.7.10 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.7m -DPYTHON_LIBRARY:FILEPATH=/usr/lib/x86_64-linux-gnu/libpython3.7m.so -DSKBUILD:BOOL=TRUE -DCMAKE_MODULE_PATH:PATH=/usr/local/lib/python3.7/dist-packages/skbuild/resources/cmake -DUSE_CUDA=ON '-DRPU_CUDA_ARCHITECTURES=60;70' -DCMAKE_BUILD_TYPE:STRING=Release\n","\n","copying _skbuild/linux-x86_64-3.7/cmake-install/src/aihwkit/simulator/rpu_base.cpython-37m-x86_64-linux-gnu.so -> src/aihwkit/simulator/rpu_base.cpython-37m-x86_64-linux-gnu.so\n","\n","running build_ext\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cftnglR9N5Sj"},"source":["# Testing Complied Library Aihwkit"]},{"cell_type":"markdown","metadata":{"id":"Z479FX64M301"},"source":["# Testing Sample Code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjmuBtTNMoOP","executionInfo":{"elapsed":1202,"status":"ok","timestamp":1618485453597,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"d9f5544f-b0fd-414a-9beb-0fa462917654"},"source":["# Writing the TestAihwkit.py file to /contents directory\n","\n","%%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/TestAihwkit.py\n","# %%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/TestAihwkit.py\n","\n","# All necessary imports\n","import os\n","\n","from torch import Tensor\n","from aihwkit.nn import AnalogLinear\n","from aihwkit.optim import AnalogSGD\n","from torch.nn.functional import mse_loss\n","\n","# Running some sample code \n","x = Tensor([[0.1, 0.2, 0.4, 0.3], [0.2, 0.1, 0.1, 0.3]])\n","y = Tensor([[1.0, 0.5], [0.7, 0.3]])\n","\n","model = AnalogLinear(4, 2)\n","optimizer = AnalogSGD(model.parameters(), lr=0.1)\n","optimizer.regroup_param_groups(model)\n","\n","for epoch in range(10):\n","    pred = model(x)\n","    loss = mse_loss(pred, y)\n","    loss.backward()\n","    optimizer.step()\n","    print(\"Loss error: \" + str(loss))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/TestAihwkit.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL68cZhOVK33","executionInfo":{"elapsed":2087,"status":"ok","timestamp":1618485478573,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"13ec0406-1b0f-4296-ffa2-a8b5840391f0"},"source":["# Now trying to run the test code from outside the aihwkit virtual environment\n","# Note: the python module search path has already been added in the cell above \n","# This should work....\n","\n","%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/\n","# ls\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/TestAihwkit.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/venv\n","/content/gdrive/MyDrive/venv/aihwkit_env/aihwkit\n"],"name":"stdout"},{"output_type":"stream","text":["python3: can't open file 'examples/TestAihwkit.py': [Errno 2] No such file or directory\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FfqPcMCCaDsw"},"source":["# ECSE 552 final: super convergence\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9OQxJFh1VLud"},"source":["## CIFAR10-ResNet18\n","\n","For more device settings, please refer to: https://aihwkit.readthedocs.io/en/latest/api/aihwkit.simulator.presets.devices.html#aihwkit.simulator.presets.devices.GokmenVlasovPresetDevice\n","\n","More on RPU_CONFIG: https://aihwkit.readthedocs.io/en/latest/api/aihwkit.simulator.presets.configs.html?highlight=EcRamPreset#aihwkit.simulator.presets.configs.EcRamPreset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yyCiKYqL9zCH"},"source":["### Write file - super convergence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xAbfRHuaKVB","executionInfo":{"elapsed":619,"status":"ok","timestamp":1618589291521,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"27965542-739d-45e7-c4e5-c60dce0230da"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_SC.py\n","%%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_SC.py\n","\n","# Importing time\n","from time import time\n","from datetime import datetime\n","\n","# Setting the correct python path\n","import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch.nn.functional as F\n","import torch\n","from torchsummary import summary\n","\n","from torch import nn\n","from torch.optim.lr_scheduler import CyclicLR\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogLinear, AnalogConv2d, AnalogSequential\n","\n","from aihwkit.optim import AnalogSGD\n","# check\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","\n","\n","# Path where the datasets will be stored.\n","TRAIN_DATASET = 'data/TRAIN_DATASET'\n","TEST_DATASET = 'data/TEST_DATASET'\n","PATH_DATASET = os.path.join('data', 'DATASET')\n","\n","\n","# Check device\n","USE_CUDA = 0\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","\n","# Path to store results\n","RESULTS = os.path.join(os.getcwd(), 'results', 'ResNet18 with CIFAR10 dataset')\n","\n","\n","# Training parameters\n","N_EPOCHS = 24\n","BATCH_SIZE = 512\n","lr = 0.1\n","SEED = 1\n","N_CLASSES = 10\n","\n","# # baseline noise setting\n","# # Create RPU configuration with noise\n","# # Noise of the weight computations\n","# RPU_CONFIG = SingleRPUConfig(device=ExpStepDevice())\n","# RPU_CONFIG.forward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# RPU_CONFIG.forward.w_noise = 0.02\n","# RPU_CONFIG.backward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# RPU_CONFIG.backward.w_noise = 0.02\n","\n","\n","\n","RPU_CONFIG = GokmenVlasovPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","\n","# with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 82.73%\tTest accuracy: 80.00% with omega = 0.8\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","\n","# with GokmenVlasovPreset()\n","# RPU_CONFIG = GokmenVlasovPreset()\n","# Test accuracy: 62.25%\tTest accuracy: 50.31% with omega = 1\n","\n","\n","\n","\n","def load_images_CIFAR10():\n","    \"\"\"Load CIFAR10 images for train from the torchvision datasets.\"\"\"\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    train_set = datasets.CIFAR10(root=TRAIN_DATASET, train=True, download=True, transform=transform_train)\n","    val_set = datasets.CIFAR10(root=TEST_DATASET, train=False, download=True, transform=transform_test)\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    print (\"==> Finish data preparation!\")\n","\n","    return train_data, validation_data\n","\n","    np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","\n","def train_step(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","\n","    for images, labels in train_data:\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, epoch_loss\n","\n","\n","\n","def test_evaluation(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, accuracy, error, epoch_loss\n","\n","def training_loop(model, criterion, optimizer, train_data, validation_data, epochs, print_every=1):\n","    \"\"\"Training loop.\n","\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    scheduler = CyclicLR(optimizer, base_lr = 0.01, max_lr= 0.5 , step_size_up= N_EPOCHS/2 , cycle_momentum=True, base_momentum=0.95, max_momentum=0.85)\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","\n","        print (\"epoch\", epoch)\n","        # Train_step\n","        model, training_acc, train_loss = train_step(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        # Decay learning rate if needed.\n","        scheduler.step()\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, accuracy, error, valid_loss = test_evaluation(validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t'\n","                  )\n","\n","      \n","    ## Save results and plot figures\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error, testing_acc)\n","\n","\n","def plot_results(train_losses, valid_losses, test_error, testing_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(testing_acc, 'r-s')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_accuracy.png'))\n","    plt.close()\n","\n","\n","class Block(nn.Module):\n","    \"\"\"Block used in building ResNet\"\"\"\n","    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n","        assert num_layers in [18, 34, 50, 101, 152], \"should be a valid architecture\"\n","\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","\n","        if self.num_layers > 34:\n","          # ResNet50, 101, and 152 include additional layer of 1x1 kernels\n","\n","          self.expansion = 4\n","          self.layer = AnalogSequential( \n","                        ## additional layer of 1x1 kernels\n","                        AnalogConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0,rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","\n","                        ### same as lower layer\n","                        AnalogConv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","                        AnalogConv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0,rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels * self.expansion)\n","                        ).cuda()\n","        else:\n","          # for ResNet18 and 34, connect input directly to (3x3) kernel (skip first (1x1))\n","          self.expansion = 1\n","          self.layer = AnalogSequential( \n","                        AnalogConv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","                        AnalogConv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels * self.expansion)\n","                        ).cuda()\n","\n","        self.identity_downsample = identity_downsample\n","        self.relu_block  = AnalogSequential( nn.ReLU() ).cuda()\n","\n","\n","    def forward(self, x):\n","        identity = x\n","        x = self.layer(x)\n","\n","        if self.identity_downsample is not None:\n","            identity = self.identity_downsample(identity)\n","\n","        x += identity\n","        x = self.relu_block(x)\n","        return x \n","   \n","class ResNet(nn.Module):\n","    \"\"\"LeNet5 inspired analog model.\"\"\"\n","    def __init__(self, num_layers, block, image_channels, num_classes):\n","        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n","                                                        f'to be 18, 34, 50, 101, or 152 '\n","        super().__init__()\n","\n","        if num_layers < 50:\n","            self.expansion = 1\n","        else:\n","            self.expansion = 4\n","\n","        if num_layers == 18:\n","            layers = [2, 2, 2, 2]\n","        elif num_layers == 34 or num_layers == 50:\n","            layers = [3, 4, 6, 3]\n","        elif num_layers == 101:\n","            layers = [3, 4, 23, 3]\n","        else:\n","            layers = [3, 8, 36, 3]\n","\n","        self.in_channels = 64\n","\n","        self.layer0 = AnalogSequential( \n","            AnalogConv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        ).cuda()\n","\n","    \n","        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n","        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n","        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n","        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n","\n","        self.fc = AnalogSequential( \n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Flatten(),\n","            AnalogLinear(in_features=512 * self.expansion, out_features=num_classes, bias=True, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA) \n","            ).cuda()\n","       \n","    def forward(self, x):\n","        x = self.layer0(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # x = self.avgpool(x)\n","        # x = x.reshape(x.shape[0], -1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n","        layers = []\n","\n","        identity_downsample = AnalogSequential(\n","                                AnalogConv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                                nn.BatchNorm2d(intermediate_channels*self.expansion)\n","                                ).cuda()\n","        \n","        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n","        self.in_channels = intermediate_channels * self.expansion # 256\n","        for i in range(num_residual_blocks - 1):\n","            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n","        return AnalogSequential(*layers).cuda()\n","\n","\n","def main():\n","    \"\"\"Super convergence on ResNet with CIFAR10 \"\"\"\n","\n","    os.makedirs(RESULTS, exist_ok=True)\n","    # torch.manual_seed(SEED)\n","    \n","    train_data, validation_data = load_images_CIFAR10()\n","    model = ResNet(18, Block, 3, 10)\n","\n","    ## show summary of model\n","    summary(model, (3, 32, 32), 512)\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","          f'Start running!')\n","    \n","  \n","    optimizer = AnalogSGD(model.parameters(), lr=lr)\n","    optimizer.regroup_param_groups(model)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model, optimizer, _ = training_loop(model, criterion, optimizer, train_data, validation_data, N_EPOCHS, print_every= 1)\n","\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","          f'Complete running!')\n","\n","\n","if __name__ == '__main__':\n","    # Execute only if run as the entry point into the program.\n","    main()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_SC.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9z24ElXQaGLh"},"source":["### Write file - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sKcpdVTaE4p","executionInfo":{"elapsed":3468,"status":"ok","timestamp":1618597049372,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"25f6610d-9073-492f-d5ff-5d14ddaf2009"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_SC.py\n","%%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_normal.py\n","\n","# Importing time\n","from time import time\n","from datetime import datetime\n","\n","# Setting the correct python path\n","import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch.nn.functional as F\n","import torch\n","from torchsummary import summary\n","\n","from torch import nn\n","from torch.optim.lr_scheduler import CyclicLR\n","from torch.optim.lr_scheduler import StepLR\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogLinear, AnalogConv2d, AnalogSequential\n","\n","from aihwkit.optim import AnalogSGD\n","# check\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","\n","\n","# Path where the datasets will be stored.\n","TRAIN_DATASET = 'data/TRAIN_DATASET'\n","TEST_DATASET = 'data/TEST_DATASET'\n","PATH_DATASET = os.path.join('data', 'DATASET')\n","\n","\n","# Check device\n","USE_CUDA = 0\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","\n","# Path to store results\n","RESULTS = os.path.join(os.getcwd(), 'results', 'ResNet18 with CIFAR10 dataset')\n","\n","\n","# Training parameters\n","N_EPOCHS = 50\n","BATCH_SIZE = 512\n","lr = 0.01\n","SEED = 1\n","N_CLASSES = 10\n","\n","# # baseline noise setting\n","# # Create RPU configuration with noise\n","# # Noise of the weight computations\n","# RPU_CONFIG = SingleRPUConfig(device=ExpStepDevice())\n","# RPU_CONFIG.forward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# RPU_CONFIG.forward.w_noise = 0.02\n","# RPU_CONFIG.backward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# RPU_CONFIG.backward.w_noise = 0.02\n","\n","\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","\n","# with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 82.73%\tTest accuracy: 80.00% with omega = 0.8\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","\n","# with GokmenVlasovPreset()\n","# RPU_CONFIG = GokmenVlasovPreset()\n","# Test accuracy: 62.25%\tTest accuracy: 50.31% with omega = 1\n","\n","\n","\n","\n","def load_images_CIFAR10():\n","    \"\"\"Load CIFAR10 images for train from the torchvision datasets.\"\"\"\n","\n","    print('==> Preparing data..')\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    train_set = datasets.CIFAR10(root=TRAIN_DATASET, train=True, download=True, transform=transform_train)\n","    val_set = datasets.CIFAR10(root=TEST_DATASET, train=False, download=True, transform=transform_test)\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    print (\"==> Finish data preparation!\")\n","\n","    return train_data, validation_data\n","\n","    np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","\n","def train_step(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","\n","    for images, labels in train_data:\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, epoch_loss\n","\n","\n","\n","def test_evaluation(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, accuracy, error, epoch_loss\n","\n","def training_loop(model, criterion, optimizer, train_data, validation_data, epochs, print_every=1):\n","    \"\"\"Training loop.\n","\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # scheduler = CyclicLR(optimizer, base_lr = 0.01, max_lr= 0.5 , step_size_up= N_EPOCHS/2 , cycle_momentum=True, base_momentum=0.95, max_momentum=0.85)\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","\n","        print (\"epoch\", epoch)\n","        # Train_step\n","        model, training_acc, train_loss = train_step(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        # Decay learning rate if needed.\n","        # scheduler.step()\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, accuracy, error, valid_loss = test_evaluation(validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t'\n","                  )\n","\n","      \n","    ## Save results and plot figures\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error, testing_acc)\n","\n","\n","def plot_results(train_losses, valid_losses, test_error, testing_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(testing_acc, 'r-s')\n","    plt.title('aihwkit ResNet18 with CIFAR10 dataset')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'test_accuracy.png'))\n","    plt.close()\n","\n","\n","class Block(nn.Module):\n","    \"\"\"Block used in building ResNet\"\"\"\n","    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n","        assert num_layers in [18, 34, 50, 101, 152], \"should be a valid architecture\"\n","\n","        super().__init__()\n","\n","        self.num_layers = num_layers\n","\n","        if self.num_layers > 34:\n","          # ResNet50, 101, and 152 include additional layer of 1x1 kernels\n","\n","          self.expansion = 4\n","          self.layer = AnalogSequential( \n","                        ## additional layer of 1x1 kernels\n","                        AnalogConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0,rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","\n","                        ### same as lower layer\n","                        AnalogConv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","                        AnalogConv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0,rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels * self.expansion)\n","                        ).cuda()\n","        else:\n","          # for ResNet18 and 34, connect input directly to (3x3) kernel (skip first (1x1))\n","          self.expansion = 1\n","          self.layer = AnalogSequential( \n","                        AnalogConv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels),\n","                        nn.ReLU(),\n","                        AnalogConv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                        nn.BatchNorm2d(out_channels * self.expansion)\n","                        ).cuda()\n","\n","        self.identity_downsample = identity_downsample\n","        self.relu_block  = AnalogSequential( nn.ReLU() ).cuda()\n","\n","\n","    def forward(self, x):\n","        identity = x\n","        x = self.layer(x)\n","\n","        if self.identity_downsample is not None:\n","            identity = self.identity_downsample(identity)\n","\n","        x += identity\n","        x = self.relu_block(x)\n","        return x \n","   \n","class ResNet(nn.Module):\n","    \"\"\"LeNet5 inspired analog model.\"\"\"\n","    def __init__(self, num_layers, block, image_channels, num_classes):\n","        assert num_layers in [18, 34, 50, 101, 152], f'ResNet{num_layers}: Unknown architecture! Number of layers has ' \\\n","                                                        f'to be 18, 34, 50, 101, or 152 '\n","        super().__init__()\n","\n","        if num_layers < 50:\n","            self.expansion = 1\n","        else:\n","            self.expansion = 4\n","\n","        if num_layers == 18:\n","            layers = [2, 2, 2, 2]\n","        elif num_layers == 34 or num_layers == 50:\n","            layers = [3, 4, 6, 3]\n","        elif num_layers == 101:\n","            layers = [3, 4, 23, 3]\n","        else:\n","            layers = [3, 8, 36, 3]\n","\n","        self.in_channels = 64\n","\n","        self.layer0 = AnalogSequential( \n","            AnalogConv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        ).cuda()\n","\n","    \n","        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n","        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n","        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n","        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n","\n","        self.fc = AnalogSequential( \n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Flatten(),\n","            AnalogLinear(in_features=512 * self.expansion, out_features=num_classes, bias=True, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA) \n","            ).cuda()\n","       \n","    def forward(self, x):\n","        x = self.layer0(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # x = self.avgpool(x)\n","        # x = x.reshape(x.shape[0], -1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n","        layers = []\n","\n","        identity_downsample = AnalogSequential(\n","                                AnalogConv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride, rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","                                nn.BatchNorm2d(intermediate_channels*self.expansion)\n","                                ).cuda()\n","        \n","        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n","        self.in_channels = intermediate_channels * self.expansion # 256\n","        for i in range(num_residual_blocks - 1):\n","            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n","        return AnalogSequential(*layers).cuda()\n","\n","\n","def main():\n","    \"\"\"Super convergence on ResNet with CIFAR10 \"\"\"\n","\n","    os.makedirs(RESULTS, exist_ok=True)\n","    # torch.manual_seed(SEED)\n","    \n","    train_data, validation_data = load_images_CIFAR10()\n","    model = ResNet(18, Block, 3, 10)\n","\n","    ## show summary of model\n","    summary(model, (3, 32, 32), 512)\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","          f'Start running!')\n","    \n","  \n","    optimizer = AnalogSGD(model.parameters(), lr=lr)\n","    optimizer.regroup_param_groups(model)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    model, optimizer, _ = training_loop(model, criterion, optimizer, train_data, validation_data, N_EPOCHS, print_every= 1)\n","\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","          f'Complete running!')\n","\n","\n","if __name__ == '__main__':\n","    # Execute only if run as the entry point into the program.\n","    main()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/ResNet18_normal.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q08Ui9hQap7z"},"source":["### Run code - super convergence"]},{"cell_type":"code","metadata":{"id":"ojcJDo1laqxA"},"source":["# Running the above code\n","\n","%%script bash\n","\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/ResNet18_SC.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"araiRvrB-j-U"},"source":["### Run code - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F1LWKmqR1k7U","executionInfo":{"elapsed":1542882,"status":"ok","timestamp":1618598623741,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"d6547411-9a26-4858-a2cf-8771927c0b82"},"source":["# Running the above code\n","\n","%%script bash\n","\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/ResNet18_normal.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final\n","/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit\n","==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Finish data preparation!\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","      AnalogConv2d-1          [512, 64, 16, 16]           9,472\n","       BatchNorm2d-2          [512, 64, 16, 16]             128\n","              ReLU-3          [512, 64, 16, 16]               0\n","         MaxPool2d-4            [512, 64, 8, 8]               0\n","      AnalogConv2d-5            [512, 64, 8, 8]          36,928\n","       BatchNorm2d-6            [512, 64, 8, 8]             128\n","              ReLU-7            [512, 64, 8, 8]               0\n","      AnalogConv2d-8            [512, 64, 8, 8]           4,160\n","       BatchNorm2d-9            [512, 64, 8, 8]             128\n","     AnalogConv2d-10            [512, 64, 8, 8]           4,160\n","      BatchNorm2d-11            [512, 64, 8, 8]             128\n","             ReLU-12            [512, 64, 8, 8]               0\n","            Block-13            [512, 64, 8, 8]               0\n","     AnalogConv2d-14            [512, 64, 8, 8]          36,928\n","      BatchNorm2d-15            [512, 64, 8, 8]             128\n","             ReLU-16            [512, 64, 8, 8]               0\n","     AnalogConv2d-17            [512, 64, 8, 8]           4,160\n","      BatchNorm2d-18            [512, 64, 8, 8]             128\n","             ReLU-19            [512, 64, 8, 8]               0\n","            Block-20            [512, 64, 8, 8]               0\n","     AnalogConv2d-21           [512, 128, 4, 4]          73,856\n","      BatchNorm2d-22           [512, 128, 4, 4]             256\n","             ReLU-23           [512, 128, 4, 4]               0\n","     AnalogConv2d-24           [512, 128, 4, 4]          16,512\n","      BatchNorm2d-25           [512, 128, 4, 4]             256\n","     AnalogConv2d-26           [512, 128, 4, 4]           8,320\n","      BatchNorm2d-27           [512, 128, 4, 4]             256\n","             ReLU-28           [512, 128, 4, 4]               0\n","            Block-29           [512, 128, 4, 4]               0\n","     AnalogConv2d-30           [512, 128, 4, 4]         147,584\n","      BatchNorm2d-31           [512, 128, 4, 4]             256\n","             ReLU-32           [512, 128, 4, 4]               0\n","     AnalogConv2d-33           [512, 128, 4, 4]          16,512\n","      BatchNorm2d-34           [512, 128, 4, 4]             256\n","             ReLU-35           [512, 128, 4, 4]               0\n","            Block-36           [512, 128, 4, 4]               0\n","     AnalogConv2d-37           [512, 256, 2, 2]         295,168\n","      BatchNorm2d-38           [512, 256, 2, 2]             512\n","             ReLU-39           [512, 256, 2, 2]               0\n","     AnalogConv2d-40           [512, 256, 2, 2]          65,792\n","      BatchNorm2d-41           [512, 256, 2, 2]             512\n","     AnalogConv2d-42           [512, 256, 2, 2]          33,024\n","      BatchNorm2d-43           [512, 256, 2, 2]             512\n","             ReLU-44           [512, 256, 2, 2]               0\n","            Block-45           [512, 256, 2, 2]               0\n","     AnalogConv2d-46           [512, 256, 2, 2]         590,080\n","      BatchNorm2d-47           [512, 256, 2, 2]             512\n","             ReLU-48           [512, 256, 2, 2]               0\n","     AnalogConv2d-49           [512, 256, 2, 2]          65,792\n","      BatchNorm2d-50           [512, 256, 2, 2]             512\n","             ReLU-51           [512, 256, 2, 2]               0\n","            Block-52           [512, 256, 2, 2]               0\n","     AnalogConv2d-53           [512, 512, 1, 1]       1,180,160\n","      BatchNorm2d-54           [512, 512, 1, 1]           1,024\n","             ReLU-55           [512, 512, 1, 1]               0\n","     AnalogConv2d-56           [512, 512, 1, 1]         262,656\n","      BatchNorm2d-57           [512, 512, 1, 1]           1,024\n","     AnalogConv2d-58           [512, 512, 1, 1]         131,584\n","      BatchNorm2d-59           [512, 512, 1, 1]           1,024\n","             ReLU-60           [512, 512, 1, 1]               0\n","            Block-61           [512, 512, 1, 1]               0\n","     AnalogConv2d-62           [512, 512, 1, 1]       2,359,808\n","      BatchNorm2d-63           [512, 512, 1, 1]           1,024\n","             ReLU-64           [512, 512, 1, 1]               0\n","     AnalogConv2d-65           [512, 512, 1, 1]         262,656\n","      BatchNorm2d-66           [512, 512, 1, 1]           1,024\n","             ReLU-67           [512, 512, 1, 1]               0\n","            Block-68           [512, 512, 1, 1]               0\n","AdaptiveAvgPool2d-69           [512, 512, 1, 1]               0\n","          Flatten-70                 [512, 512]               0\n","     AnalogLinear-71                  [512, 10]           5,130\n","================================================================\n","Total params: 5,620,170\n","Trainable params: 5,620,170\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 6.00\n","Forward/backward pass size (MB): 692.04\n","Params size (MB): 21.44\n","Estimated Total Size (MB): 719.48\n","----------------------------------------------------------------\n","\n","18:19:31 --- Start running!\n","epoch 0\n","\n","18:20:09 --- Epoch: 0\tTrain loss: 1.9281\tValid loss: 1.7072\tTraining accuracy: 28.92%\tTest accuracy: 37.13%\t\n","epoch 1\n","\n","18:20:37 --- Epoch: 1\tTrain loss: 1.6306\tValid loss: 1.5015\tTraining accuracy: 39.74%\tTest accuracy: 45.16%\t\n","epoch 2\n","\n","18:21:05 --- Epoch: 2\tTrain loss: 1.5243\tValid loss: 1.4356\tTraining accuracy: 44.22%\tTest accuracy: 47.69%\t\n","epoch 3\n","\n","18:21:33 --- Epoch: 3\tTrain loss: 1.4532\tValid loss: 1.4520\tTraining accuracy: 46.93%\tTest accuracy: 46.69%\t\n","epoch 4\n","\n","18:22:01 --- Epoch: 4\tTrain loss: 1.3988\tValid loss: 1.3715\tTraining accuracy: 49.13%\tTest accuracy: 50.54%\t\n","epoch 5\n","\n","18:22:28 --- Epoch: 5\tTrain loss: 1.3559\tValid loss: 1.3331\tTraining accuracy: 50.74%\tTest accuracy: 51.38%\t\n","epoch 6\n","\n","18:22:55 --- Epoch: 6\tTrain loss: 1.3192\tValid loss: 1.2873\tTraining accuracy: 52.10%\tTest accuracy: 53.66%\t\n","epoch 7\n","\n","18:23:22 --- Epoch: 7\tTrain loss: 1.2888\tValid loss: 1.2619\tTraining accuracy: 53.23%\tTest accuracy: 54.64%\t\n","epoch 8\n","\n","18:23:50 --- Epoch: 8\tTrain loss: 1.2526\tValid loss: 1.3411\tTraining accuracy: 54.62%\tTest accuracy: 52.59%\t\n","epoch 9\n","\n","18:24:18 --- Epoch: 9\tTrain loss: 1.2196\tValid loss: 1.2103\tTraining accuracy: 55.81%\tTest accuracy: 56.33%\t\n","epoch 10\n","\n","18:24:46 --- Epoch: 10\tTrain loss: 1.1942\tValid loss: 1.1560\tTraining accuracy: 56.66%\tTest accuracy: 58.15%\t\n","epoch 11\n","\n","18:25:15 --- Epoch: 11\tTrain loss: 1.1642\tValid loss: 1.1485\tTraining accuracy: 57.92%\tTest accuracy: 58.07%\t\n","epoch 12\n","\n","18:25:43 --- Epoch: 12\tTrain loss: 1.1387\tValid loss: 1.1484\tTraining accuracy: 58.93%\tTest accuracy: 58.77%\t\n","epoch 13\n","\n","18:26:11 --- Epoch: 13\tTrain loss: 1.1149\tValid loss: 1.0918\tTraining accuracy: 60.00%\tTest accuracy: 61.03%\t\n","epoch 14\n","\n","18:26:39 --- Epoch: 14\tTrain loss: 1.0942\tValid loss: 1.1881\tTraining accuracy: 60.74%\tTest accuracy: 57.39%\t\n","epoch 15\n","\n","18:27:08 --- Epoch: 15\tTrain loss: 1.0768\tValid loss: 1.0915\tTraining accuracy: 61.21%\tTest accuracy: 60.55%\t\n","epoch 16\n","\n","18:27:37 --- Epoch: 16\tTrain loss: 1.0617\tValid loss: 1.1636\tTraining accuracy: 61.74%\tTest accuracy: 58.61%\t\n","epoch 17\n","\n","18:28:05 --- Epoch: 17\tTrain loss: 1.0431\tValid loss: 1.0182\tTraining accuracy: 62.20%\tTest accuracy: 63.56%\t\n","epoch 18\n","\n","18:28:34 --- Epoch: 18\tTrain loss: 1.0210\tValid loss: 1.0242\tTraining accuracy: 63.28%\tTest accuracy: 63.03%\t\n","epoch 19\n","\n","18:29:03 --- Epoch: 19\tTrain loss: 1.0049\tValid loss: 1.0862\tTraining accuracy: 63.92%\tTest accuracy: 62.01%\t\n","epoch 20\n","\n","18:29:31 --- Epoch: 20\tTrain loss: 0.9913\tValid loss: 1.0102\tTraining accuracy: 64.57%\tTest accuracy: 64.32%\t\n","epoch 21\n","\n","18:30:00 --- Epoch: 21\tTrain loss: 0.9695\tValid loss: 1.1136\tTraining accuracy: 65.28%\tTest accuracy: 61.01%\t\n","epoch 22\n","\n","18:30:29 --- Epoch: 22\tTrain loss: 0.9627\tValid loss: 1.0115\tTraining accuracy: 65.44%\tTest accuracy: 64.12%\t\n","epoch 23\n","\n","18:30:58 --- Epoch: 23\tTrain loss: 0.9416\tValid loss: 1.0088\tTraining accuracy: 66.27%\tTest accuracy: 64.69%\t\n","epoch 24\n","\n","18:31:27 --- Epoch: 24\tTrain loss: 0.9313\tValid loss: 1.0012\tTraining accuracy: 66.74%\tTest accuracy: 64.84%\t\n","epoch 25\n","\n","18:31:55 --- Epoch: 25\tTrain loss: 0.9235\tValid loss: 0.9617\tTraining accuracy: 66.95%\tTest accuracy: 66.14%\t\n","epoch 26\n","\n","18:32:24 --- Epoch: 26\tTrain loss: 0.9095\tValid loss: 0.9632\tTraining accuracy: 67.23%\tTest accuracy: 66.25%\t\n","epoch 27\n","\n","18:32:54 --- Epoch: 27\tTrain loss: 0.8912\tValid loss: 0.9685\tTraining accuracy: 68.00%\tTest accuracy: 66.41%\t\n","epoch 28\n","\n","18:33:23 --- Epoch: 28\tTrain loss: 0.8841\tValid loss: 0.9524\tTraining accuracy: 68.38%\tTest accuracy: 66.14%\t\n","epoch 29\n","\n","18:33:52 --- Epoch: 29\tTrain loss: 0.8726\tValid loss: 0.9124\tTraining accuracy: 68.44%\tTest accuracy: 67.96%\t\n","epoch 30\n","\n","18:34:21 --- Epoch: 30\tTrain loss: 0.8663\tValid loss: 0.9057\tTraining accuracy: 69.05%\tTest accuracy: 67.96%\t\n","epoch 31\n","\n","18:34:50 --- Epoch: 31\tTrain loss: 0.8514\tValid loss: 0.9306\tTraining accuracy: 69.63%\tTest accuracy: 67.54%\t\n","epoch 32\n","\n","18:35:19 --- Epoch: 32\tTrain loss: 0.8406\tValid loss: 0.8730\tTraining accuracy: 69.86%\tTest accuracy: 69.47%\t\n","epoch 33\n","\n","18:35:48 --- Epoch: 33\tTrain loss: 0.8370\tValid loss: 0.8958\tTraining accuracy: 70.21%\tTest accuracy: 68.51%\t\n","epoch 34\n","\n","18:36:17 --- Epoch: 34\tTrain loss: 0.8272\tValid loss: 0.8724\tTraining accuracy: 70.46%\tTest accuracy: 69.90%\t\n","epoch 35\n","\n","18:36:47 --- Epoch: 35\tTrain loss: 0.8159\tValid loss: 0.8891\tTraining accuracy: 70.84%\tTest accuracy: 69.04%\t\n","epoch 36\n","\n","18:37:16 --- Epoch: 36\tTrain loss: 0.8062\tValid loss: 0.8605\tTraining accuracy: 71.13%\tTest accuracy: 69.45%\t\n","epoch 37\n","\n","18:37:45 --- Epoch: 37\tTrain loss: 0.7958\tValid loss: 0.9384\tTraining accuracy: 71.54%\tTest accuracy: 67.14%\t\n","epoch 38\n","\n","18:38:15 --- Epoch: 38\tTrain loss: 0.7869\tValid loss: 0.8678\tTraining accuracy: 71.85%\tTest accuracy: 69.79%\t\n","epoch 39\n","\n","18:38:44 --- Epoch: 39\tTrain loss: 0.7805\tValid loss: 0.8697\tTraining accuracy: 71.95%\tTest accuracy: 69.28%\t\n","epoch 40\n","\n","18:39:13 --- Epoch: 40\tTrain loss: 0.7760\tValid loss: 0.8724\tTraining accuracy: 72.31%\tTest accuracy: 69.47%\t\n","epoch 41\n","\n","18:39:42 --- Epoch: 41\tTrain loss: 0.7661\tValid loss: 0.8430\tTraining accuracy: 72.59%\tTest accuracy: 70.12%\t\n","epoch 42\n","\n","18:40:11 --- Epoch: 42\tTrain loss: 0.7558\tValid loss: 0.8590\tTraining accuracy: 72.88%\tTest accuracy: 69.88%\t\n","epoch 43\n","\n","18:40:40 --- Epoch: 43\tTrain loss: 0.7529\tValid loss: 0.8538\tTraining accuracy: 73.15%\tTest accuracy: 70.01%\t\n","epoch 44\n","\n","18:41:08 --- Epoch: 44\tTrain loss: 0.7445\tValid loss: 0.8312\tTraining accuracy: 73.29%\tTest accuracy: 70.85%\t\n","epoch 45\n","\n","18:41:37 --- Epoch: 45\tTrain loss: 0.7380\tValid loss: 0.8068\tTraining accuracy: 73.62%\tTest accuracy: 71.72%\t\n","epoch 46\n","\n","18:42:06 --- Epoch: 46\tTrain loss: 0.7263\tValid loss: 0.8130\tTraining accuracy: 74.08%\tTest accuracy: 71.79%\t\n","epoch 47\n","\n","18:42:34 --- Epoch: 47\tTrain loss: 0.7256\tValid loss: 0.8116\tTraining accuracy: 74.25%\tTest accuracy: 71.41%\t\n","epoch 48\n","\n","18:43:03 --- Epoch: 48\tTrain loss: 0.7177\tValid loss: 0.8594\tTraining accuracy: 74.33%\tTest accuracy: 70.13%\t\n","epoch 49\n","\n","18:43:31 --- Epoch: 49\tTrain loss: 0.7041\tValid loss: 0.8013\tTraining accuracy: 74.82%\tTest accuracy: 72.02%\t\n","18:43:42 --- Complete running!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_iEXIAs8D_Ob"},"source":["## MNIST - LeNet\n","\n","TODO: find when super convergence happens"]},{"cell_type":"markdown","metadata":{"id":"GTnlrpgA9BFG"},"source":["### Write file - super convergence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdLqmfUmnrK-","executionInfo":{"status":"ok","timestamp":1618844205995,"user_tz":-180,"elapsed":1615,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"}},"outputId":"3e9efefb-a7d7-4177-9b85-976f3a3a22e6"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n","# %%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n","%%writefile /content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","# SEED = 1\n","N_EPOCHS = 85\n","BATCH_SIZE = 512\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60% Test accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47% Test accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05% Test accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# ***NOTE*** change to your own path before running\n","# set paths for training and validation data and results\n","PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_data\"\n","# PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_data\"\n","RESULTS = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_aihwkit_results/stepLR\"\n","# RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_aihwkit_results\"\n","\n","# Path to store results\n","# RESULTS = os.path.join(os.getcwd(), 'results', 'LeNet with MNIST dataset')\n","# Path where the datasets will be stored.\n","TRAIN_DATASET = 'data/TRAIN_DATASET'\n","TEST_DATASET = 'data/TEST_DATASET'\n","# PATH_DATASET = os.path.join('data', 'DATASET')\n","ADD_PATH = \"_NSC_GI_85Epoch_T1.csv\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","  path = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_aihwkit_results/stepLR\"\n","  # path = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","# load data\n","def load_images_MNIST():\n","    \"\"\"Load images for train from the torchvision datasets.\"\"\"\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    # Load the images.\n","    train_set = datasets.MNIST(TRAIN_DATASET,\n","                               download=True, train=True, transform=transform)\n","    val_set = datasets.MNIST(TEST_DATASET,\n","                             download=True, train=False, transform=transform)\n","    train_data = torch.utils.data.DataLoader(\n","        train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(\n","        val_set, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class LeNet(nn.Sequential):\n","  def __init__(self, rpu_config, num_classes, weight_scaling_omega) -> None:\n","    super().__init__()\n","    self.features = AnalogSequential(\n","        AnalogConv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1,\n","                      rpu_config=rpu_config, weight_scaling_omega=weight_scaling_omega),\n","        nn.Tanh(),\n","        nn.MaxPool2d(kernel_size=2),\n","        AnalogConv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1,\n","                      rpu_config=rpu_config, weight_scaling_omega = weight_scaling_omega),\n","        nn.Tanh(),\n","        nn.MaxPool2d(kernel_size=2),\n","        nn.Tanh() ).cuda()\n","    self.classification = AnalogSequential(\n","    AnalogLinear(in_features=512, out_features=128, rpu_config=rpu_config, weight_scaling_omega = weight_scaling_omega),\n","    nn.Tanh(),\n","    AnalogLinear(in_features=128, out_features=N_CLASSES, rpu_config=rpu_config, weight_scaling_omega = weight_scaling_omega),\n","    ).cuda()\n","    # self.features, self.classification = create_analog_lenet_network(rpu_config, num_classes, weight_scaling_omega)\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    logits = self.classification(x)\n","    predict = F.softmax(logits, dim=1)\n","    return predict\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        # print(accuracy)\n","        scheduler.step(valid_loss)\n","\n","    # # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error_SC\" + ADD_PATH), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses_SC\"+ ADD_PATH), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses_SC\" +  ADD_PATH), valid_losses, delimiter=\",\")\n","    # plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_losses_GI_85Epoch_T1.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_error_GI_85Epoch_T1.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_accuracy_GI_85Epoch_T1.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('LENET Aihwkit MNIST Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('LENET Aihwkit MNIST Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('LENET Aihwkit MNIST Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # create results director for simulation\n","    create_results_dir()\n","\n","    # set seed\n","    # torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_MNIST()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = LeNet(RPU_CONFIG, N_CLASSES, WEIGHT_SCALING_OMEGA)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    summary(model, (512, 28, 28), BATCH_SIZE)\n","    # exit(0)\n","    print(model)\n","\n","    # optimizer\n","    optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n","    optimizer.regroup_param_groups(model)\n","\n","    # # super convergence scheduler\n","    # scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n","    #                                           base_lr = 0.01, \n","    #                                           max_lr= 0.1, \n","    #                                           step_size_up= math.floor(N_EPOCHS/2), \n","    #                                           cycle_momentum=True, \n","    #                                           base_momentum=0.95, \n","    #                                           max_momentum=0.85)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.94)\n","    #ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose = True)\n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1)\n","    # \n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1)\n","    #lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=9, verbose = True, threshold=0.0001, threshold_mode='abs', cooldown=1)\n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1) #StepLR(optimizer, step_size=17, gamma=0.1)\n","    \n","\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Lenet Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_sc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS, scheduler)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Lenet Example')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4VMi-31V1pU9"},"source":["### Run Code - super convergence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIV6vbQapxXE","executionInfo":{"status":"ok","timestamp":1618848205085,"user_tz":-180,"elapsed":546015,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"}},"outputId":"9013a050-10e2-4c20-e3af-1823868b831e"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/ \n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/lenet_sc_aihwkit_mnist.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/aihw_venv\n","/content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit\n","finished making dir...\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","      AnalogConv2d-1          [512, 16, 24, 24]             416\n","              Tanh-2          [512, 16, 24, 24]               0\n","         MaxPool2d-3          [512, 16, 12, 12]               0\n","      AnalogConv2d-4            [512, 32, 8, 8]          12,832\n","              Tanh-5            [512, 32, 8, 8]               0\n","         MaxPool2d-6            [512, 32, 4, 4]               0\n","              Tanh-7            [512, 32, 4, 4]               0\n","      AnalogLinear-8                 [512, 128]          65,664\n","              Tanh-9                 [512, 128]               0\n","     AnalogLinear-10                  [512, 10]           1,290\n","================================================================\n","Total params: 80,202\n","Trainable params: 80,202\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 784.00\n","Forward/backward pass size (MB): 102.04\n","Params size (MB): 0.31\n","Estimated Total Size (MB): 886.35\n","----------------------------------------------------------------\n","LeNet(\n","  (features): AnalogSequential(\n","    (0): AnalogConv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), weight_scaling_omega=0.800)\n","    (1): Tanh()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): AnalogConv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), weight_scaling_omega=0.800)\n","    (4): Tanh()\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Tanh()\n","  )\n","  (classification): AnalogSequential(\n","    (0): AnalogLinear(in_features=512, out_features=128, bias=True, weight_scaling_omega=0.800)\n","    (1): Tanh()\n","    (2): AnalogLinear(in_features=128, out_features=10, bias=True, weight_scaling_omega=0.800)\n","  )\n",")\n","\n","15:54:25 --- Started Lenet Example\n","15:54:37 --- Epoch: 0\tTrain loss: 2.3023\tValid loss: 2.3020\tTraining accuracy: 10.58%\tTest error: 89.26%\tTest accuracy: 10.74%\t\n","15:54:42 --- Epoch: 1\tTrain loss: 2.3017\tValid loss: 2.3014\tTraining accuracy: 11.21%\tTest error: 88.70%\tTest accuracy: 11.30%\t\n","15:54:48 --- Epoch: 2\tTrain loss: 2.3012\tValid loss: 2.3008\tTraining accuracy: 11.99%\tTest error: 87.48%\tTest accuracy: 12.52%\t\n","15:54:54 --- Epoch: 3\tTrain loss: 2.3006\tValid loss: 2.3002\tTraining accuracy: 12.83%\tTest error: 86.39%\tTest accuracy: 13.61%\t\n","15:55:00 --- Epoch: 4\tTrain loss: 2.2999\tValid loss: 2.2995\tTraining accuracy: 14.28%\tTest error: 84.78%\tTest accuracy: 15.22%\t\n","15:55:05 --- Epoch: 5\tTrain loss: 2.2993\tValid loss: 2.2988\tTraining accuracy: 16.13%\tTest error: 82.68%\tTest accuracy: 17.32%\t\n","15:55:11 --- Epoch: 6\tTrain loss: 2.2986\tValid loss: 2.2981\tTraining accuracy: 18.74%\tTest error: 79.71%\tTest accuracy: 20.29%\t\n","15:55:17 --- Epoch: 7\tTrain loss: 2.2978\tValid loss: 2.2973\tTraining accuracy: 21.63%\tTest error: 76.45%\tTest accuracy: 23.55%\t\n","15:55:23 --- Epoch: 8\tTrain loss: 2.2970\tValid loss: 2.2964\tTraining accuracy: 24.55%\tTest error: 73.53%\tTest accuracy: 26.47%\t\n","15:55:28 --- Epoch: 9\tTrain loss: 2.2961\tValid loss: 2.2954\tTraining accuracy: 27.86%\tTest error: 69.63%\tTest accuracy: 30.37%\t\n","15:55:34 --- Epoch: 10\tTrain loss: 2.2951\tValid loss: 2.2944\tTraining accuracy: 31.97%\tTest error: 65.72%\tTest accuracy: 34.28%\t\n","15:55:40 --- Epoch: 11\tTrain loss: 2.2940\tValid loss: 2.2932\tTraining accuracy: 35.79%\tTest error: 61.29%\tTest accuracy: 38.71%\t\n","15:55:45 --- Epoch: 12\tTrain loss: 2.2927\tValid loss: 2.2918\tTraining accuracy: 40.02%\tTest error: 57.30%\tTest accuracy: 42.70%\t\n","15:55:51 --- Epoch: 13\tTrain loss: 2.2913\tValid loss: 2.2902\tTraining accuracy: 43.28%\tTest error: 53.81%\tTest accuracy: 46.19%\t\n","15:55:57 --- Epoch: 14\tTrain loss: 2.2896\tValid loss: 2.2884\tTraining accuracy: 45.87%\tTest error: 51.60%\tTest accuracy: 48.40%\t\n","15:56:03 --- Epoch: 15\tTrain loss: 2.2877\tValid loss: 2.2863\tTraining accuracy: 48.17%\tTest error: 50.03%\tTest accuracy: 49.97%\t\n","15:56:08 --- Epoch: 16\tTrain loss: 2.2854\tValid loss: 2.2837\tTraining accuracy: 49.33%\tTest error: 48.57%\tTest accuracy: 51.43%\t\n","15:56:14 --- Epoch: 17\tTrain loss: 2.2827\tValid loss: 2.2806\tTraining accuracy: 50.77%\tTest error: 47.87%\tTest accuracy: 52.13%\t\n","15:56:20 --- Epoch: 18\tTrain loss: 2.2793\tValid loss: 2.2767\tTraining accuracy: 51.28%\tTest error: 47.57%\tTest accuracy: 52.43%\t\n","15:56:26 --- Epoch: 19\tTrain loss: 2.2751\tValid loss: 2.2719\tTraining accuracy: 51.55%\tTest error: 47.76%\tTest accuracy: 52.24%\t\n","15:56:31 --- Epoch: 20\tTrain loss: 2.2696\tValid loss: 2.2656\tTraining accuracy: 51.04%\tTest error: 48.58%\tTest accuracy: 51.42%\t\n","15:56:37 --- Epoch: 21\tTrain loss: 2.2624\tValid loss: 2.2570\tTraining accuracy: 50.45%\tTest error: 48.48%\tTest accuracy: 51.52%\t\n","15:56:43 --- Epoch: 22\tTrain loss: 2.2526\tValid loss: 2.2455\tTraining accuracy: 50.02%\tTest error: 48.97%\tTest accuracy: 51.03%\t\n","15:56:49 --- Epoch: 23\tTrain loss: 2.2391\tValid loss: 2.2293\tTraining accuracy: 49.79%\tTest error: 49.42%\tTest accuracy: 50.58%\t\n","15:56:55 --- Epoch: 24\tTrain loss: 2.2205\tValid loss: 2.2076\tTraining accuracy: 49.79%\tTest error: 48.84%\tTest accuracy: 51.16%\t\n","15:57:01 --- Epoch: 25\tTrain loss: 2.1961\tValid loss: 2.1794\tTraining accuracy: 50.67%\tTest error: 47.92%\tTest accuracy: 52.08%\t\n","15:57:07 --- Epoch: 26\tTrain loss: 2.1658\tValid loss: 2.1454\tTraining accuracy: 51.80%\tTest error: 46.11%\tTest accuracy: 53.89%\t\n","15:57:14 --- Epoch: 27\tTrain loss: 2.1307\tValid loss: 2.1077\tTraining accuracy: 53.57%\tTest error: 44.65%\tTest accuracy: 55.35%\t\n","15:57:20 --- Epoch: 28\tTrain loss: 2.0937\tValid loss: 2.0701\tTraining accuracy: 55.14%\tTest error: 43.44%\tTest accuracy: 56.56%\t\n","15:57:26 --- Epoch: 29\tTrain loss: 2.0574\tValid loss: 2.0337\tTraining accuracy: 56.70%\tTest error: 42.48%\tTest accuracy: 57.52%\t\n","15:57:33 --- Epoch: 30\tTrain loss: 2.0226\tValid loss: 2.0000\tTraining accuracy: 57.86%\tTest error: 41.51%\tTest accuracy: 58.49%\t\n","15:57:39 --- Epoch: 31\tTrain loss: 1.9912\tValid loss: 1.9703\tTraining accuracy: 59.15%\tTest error: 40.06%\tTest accuracy: 59.94%\t\n","15:57:46 --- Epoch: 32\tTrain loss: 1.9624\tValid loss: 1.9422\tTraining accuracy: 60.44%\tTest error: 38.49%\tTest accuracy: 61.51%\t\n","15:57:52 --- Epoch: 33\tTrain loss: 1.9359\tValid loss: 1.9169\tTraining accuracy: 62.03%\tTest error: 36.83%\tTest accuracy: 63.17%\t\n","15:57:58 --- Epoch: 34\tTrain loss: 1.9119\tValid loss: 1.8938\tTraining accuracy: 63.38%\tTest error: 35.23%\tTest accuracy: 64.77%\t\n","15:58:05 --- Epoch: 35\tTrain loss: 1.8905\tValid loss: 1.8740\tTraining accuracy: 64.77%\tTest error: 33.87%\tTest accuracy: 66.13%\t\n","15:58:11 --- Epoch: 36\tTrain loss: 1.8724\tValid loss: 1.8570\tTraining accuracy: 66.16%\tTest error: 32.31%\tTest accuracy: 67.69%\t\n","15:58:18 --- Epoch: 37\tTrain loss: 1.8565\tValid loss: 1.8419\tTraining accuracy: 67.97%\tTest error: 30.12%\tTest accuracy: 69.88%\t\n","15:58:25 --- Epoch: 38\tTrain loss: 1.8426\tValid loss: 1.8286\tTraining accuracy: 69.99%\tTest error: 28.23%\tTest accuracy: 71.77%\t\n","15:58:31 --- Epoch: 39\tTrain loss: 1.8296\tValid loss: 1.8159\tTraining accuracy: 71.91%\tTest error: 26.34%\tTest accuracy: 73.66%\t\n","15:58:38 --- Epoch: 40\tTrain loss: 1.8172\tValid loss: 1.8036\tTraining accuracy: 73.71%\tTest error: 24.72%\tTest accuracy: 75.28%\t\n","15:58:45 --- Epoch: 41\tTrain loss: 1.8050\tValid loss: 1.7910\tTraining accuracy: 75.63%\tTest error: 22.64%\tTest accuracy: 77.36%\t\n","15:58:51 --- Epoch: 42\tTrain loss: 1.7925\tValid loss: 1.7784\tTraining accuracy: 77.68%\tTest error: 20.13%\tTest accuracy: 79.87%\t\n","15:58:58 --- Epoch: 43\tTrain loss: 1.7799\tValid loss: 1.7657\tTraining accuracy: 79.78%\tTest error: 17.93%\tTest accuracy: 82.07%\t\n","15:59:04 --- Epoch: 44\tTrain loss: 1.7673\tValid loss: 1.7529\tTraining accuracy: 81.56%\tTest error: 16.34%\tTest accuracy: 83.66%\t\n","15:59:11 --- Epoch: 45\tTrain loss: 1.7549\tValid loss: 1.7409\tTraining accuracy: 83.16%\tTest error: 15.04%\tTest accuracy: 84.96%\t\n","15:59:17 --- Epoch: 46\tTrain loss: 1.7436\tValid loss: 1.7299\tTraining accuracy: 84.24%\tTest error: 14.14%\tTest accuracy: 85.86%\t\n","15:59:24 --- Epoch: 47\tTrain loss: 1.7331\tValid loss: 1.7197\tTraining accuracy: 85.09%\tTest error: 13.54%\tTest accuracy: 86.46%\t\n","15:59:30 --- Epoch: 48\tTrain loss: 1.7234\tValid loss: 1.7102\tTraining accuracy: 85.62%\tTest error: 12.87%\tTest accuracy: 87.13%\t\n","15:59:37 --- Epoch: 49\tTrain loss: 1.7145\tValid loss: 1.7016\tTraining accuracy: 86.04%\tTest error: 12.52%\tTest accuracy: 87.48%\t\n","15:59:43 --- Epoch: 50\tTrain loss: 1.7064\tValid loss: 1.6939\tTraining accuracy: 86.41%\tTest error: 12.10%\tTest accuracy: 87.90%\t\n","15:59:50 --- Epoch: 51\tTrain loss: 1.6989\tValid loss: 1.6865\tTraining accuracy: 86.75%\tTest error: 11.82%\tTest accuracy: 88.18%\t\n","15:59:56 --- Epoch: 52\tTrain loss: 1.6921\tValid loss: 1.6801\tTraining accuracy: 87.08%\tTest error: 11.70%\tTest accuracy: 88.30%\t\n","16:00:03 --- Epoch: 53\tTrain loss: 1.6859\tValid loss: 1.6736\tTraining accuracy: 87.35%\tTest error: 11.42%\tTest accuracy: 88.58%\t\n","16:00:10 --- Epoch: 54\tTrain loss: 1.6798\tValid loss: 1.6679\tTraining accuracy: 87.52%\tTest error: 11.24%\tTest accuracy: 88.76%\t\n","16:00:16 --- Epoch: 55\tTrain loss: 1.6742\tValid loss: 1.6623\tTraining accuracy: 87.84%\tTest error: 10.90%\tTest accuracy: 89.10%\t\n","16:00:23 --- Epoch: 56\tTrain loss: 1.6691\tValid loss: 1.6574\tTraining accuracy: 88.01%\tTest error: 10.78%\tTest accuracy: 89.22%\t\n","16:00:29 --- Epoch: 57\tTrain loss: 1.6644\tValid loss: 1.6531\tTraining accuracy: 88.23%\tTest error: 10.59%\tTest accuracy: 89.41%\t\n","16:00:36 --- Epoch: 58\tTrain loss: 1.6599\tValid loss: 1.6489\tTraining accuracy: 88.41%\tTest error: 10.40%\tTest accuracy: 89.60%\t\n","16:00:42 --- Epoch: 59\tTrain loss: 1.6558\tValid loss: 1.6447\tTraining accuracy: 88.65%\tTest error: 10.29%\tTest accuracy: 89.71%\t\n","16:00:49 --- Epoch: 60\tTrain loss: 1.6518\tValid loss: 1.6409\tTraining accuracy: 88.79%\tTest error: 10.14%\tTest accuracy: 89.86%\t\n","16:00:55 --- Epoch: 61\tTrain loss: 1.6480\tValid loss: 1.6373\tTraining accuracy: 88.97%\tTest error: 9.88%\tTest accuracy: 90.12%\t\n","16:01:01 --- Epoch: 62\tTrain loss: 1.6445\tValid loss: 1.6337\tTraining accuracy: 89.20%\tTest error: 9.82%\tTest accuracy: 90.18%\t\n","16:01:08 --- Epoch: 63\tTrain loss: 1.6412\tValid loss: 1.6306\tTraining accuracy: 89.35%\tTest error: 9.60%\tTest accuracy: 90.40%\t\n","16:01:15 --- Epoch: 64\tTrain loss: 1.6381\tValid loss: 1.6276\tTraining accuracy: 89.47%\tTest error: 9.60%\tTest accuracy: 90.40%\t\n","16:01:21 --- Epoch: 65\tTrain loss: 1.6351\tValid loss: 1.6249\tTraining accuracy: 89.61%\tTest error: 9.38%\tTest accuracy: 90.62%\t\n","16:01:28 --- Epoch: 66\tTrain loss: 1.6323\tValid loss: 1.6220\tTraining accuracy: 89.75%\tTest error: 9.17%\tTest accuracy: 90.83%\t\n","16:01:34 --- Epoch: 67\tTrain loss: 1.6296\tValid loss: 1.6195\tTraining accuracy: 89.88%\tTest error: 9.11%\tTest accuracy: 90.89%\t\n","16:01:40 --- Epoch: 68\tTrain loss: 1.6271\tValid loss: 1.6171\tTraining accuracy: 90.03%\tTest error: 8.97%\tTest accuracy: 91.03%\t\n","16:01:47 --- Epoch: 69\tTrain loss: 1.6246\tValid loss: 1.6145\tTraining accuracy: 90.11%\tTest error: 8.95%\tTest accuracy: 91.05%\t\n","16:01:53 --- Epoch: 70\tTrain loss: 1.6222\tValid loss: 1.6123\tTraining accuracy: 90.22%\tTest error: 8.74%\tTest accuracy: 91.26%\t\n","16:02:00 --- Epoch: 71\tTrain loss: 1.6198\tValid loss: 1.6100\tTraining accuracy: 90.32%\tTest error: 8.67%\tTest accuracy: 91.33%\t\n","16:02:06 --- Epoch: 72\tTrain loss: 1.6178\tValid loss: 1.6080\tTraining accuracy: 90.49%\tTest error: 8.62%\tTest accuracy: 91.38%\t\n","16:02:13 --- Epoch: 73\tTrain loss: 1.6156\tValid loss: 1.6059\tTraining accuracy: 90.62%\tTest error: 8.49%\tTest accuracy: 91.51%\t\n","16:02:19 --- Epoch: 74\tTrain loss: 1.6135\tValid loss: 1.6041\tTraining accuracy: 90.73%\tTest error: 8.46%\tTest accuracy: 91.54%\t\n","16:02:25 --- Epoch: 75\tTrain loss: 1.6115\tValid loss: 1.6022\tTraining accuracy: 90.78%\tTest error: 8.39%\tTest accuracy: 91.61%\t\n","16:02:32 --- Epoch: 76\tTrain loss: 1.6096\tValid loss: 1.6005\tTraining accuracy: 90.89%\tTest error: 8.32%\tTest accuracy: 91.68%\t\n","16:02:38 --- Epoch: 77\tTrain loss: 1.6078\tValid loss: 1.5988\tTraining accuracy: 90.97%\tTest error: 8.17%\tTest accuracy: 91.83%\t\n","16:02:45 --- Epoch: 78\tTrain loss: 1.6060\tValid loss: 1.5968\tTraining accuracy: 91.10%\tTest error: 8.18%\tTest accuracy: 91.82%\t\n","16:02:51 --- Epoch: 79\tTrain loss: 1.6042\tValid loss: 1.5952\tTraining accuracy: 91.20%\tTest error: 8.09%\tTest accuracy: 91.91%\t\n","16:02:57 --- Epoch: 80\tTrain loss: 1.6027\tValid loss: 1.5936\tTraining accuracy: 91.25%\tTest error: 7.94%\tTest accuracy: 92.06%\t\n","16:03:04 --- Epoch: 81\tTrain loss: 1.6010\tValid loss: 1.5920\tTraining accuracy: 91.38%\tTest error: 7.92%\tTest accuracy: 92.08%\t\n","16:03:10 --- Epoch: 82\tTrain loss: 1.5995\tValid loss: 1.5905\tTraining accuracy: 91.44%\tTest error: 7.90%\tTest accuracy: 92.10%\t\n","16:03:17 --- Epoch: 83\tTrain loss: 1.5979\tValid loss: 1.5892\tTraining accuracy: 91.55%\tTest error: 7.75%\tTest accuracy: 92.25%\t\n","16:03:23 --- Epoch: 84\tTrain loss: 1.5964\tValid loss: 1.5878\tTraining accuracy: 91.61%\tTest error: 7.67%\tTest accuracy: 92.33%\t\n","16:03:23 --- Completed Lenet Example\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cclK6ixY4MAx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bNGF2LlKhceB"},"source":["### Write file - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kSoy60nhV0f","executionInfo":{"status":"ok","timestamp":1618852527077,"user_tz":-180,"elapsed":1361,"user":{"displayName":"Amna Sajjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM06eCwjqruN0wUfda-PTPlTbHTk3jGwGxRrY3Pg=s64","userId":"03051534757007455985"}},"outputId":"4078a59e-541e-4abe-b9b9-2972bbf6692c"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n","# %%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_mnist.py\n","%%writefile /content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit/examples/lenet_nsc_aihwkit_mnist.py\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","# SEED = 1\n","N_EPOCHS = 50\n","BATCH_SIZE = 512\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","RPU_CONFIG = GokmenVlasovPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60% Test accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47% Test accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05% Test accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# ***NOTE*** change to your own path before running\n","# set paths for training and validation data and results\n","PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_data\"\n","# PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_data\"\n","RESULTS = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_aihwkit_results\"\n","# RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_aihwkit_results\"\n","\n","# Path to store results\n","# RESULTS = os.path.join(os.getcwd(), 'results', 'LeNet with MNIST dataset')\n","# Path where the datasets will be stored.\n","TRAIN_DATASET = 'data/TRAIN_DATASET'\n","TEST_DATASET = 'data/TEST_DATASET'\n","# PATH_DATASET = os.path.join('data', 'DATASET')\n","ADD_PATH = \"_SC_NO_12Epoch_T1.csv\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","  path = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_aihwkit_results\"\n","  # path = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","# load data\n","def load_images_MNIST():\n","    \"\"\"Load images for train from the torchvision datasets.\"\"\"\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    # Load the images.\n","    train_set = datasets.MNIST(TRAIN_DATASET,\n","                               download=True, train=True, transform=transform)\n","    val_set = datasets.MNIST(TEST_DATASET,\n","                             download=True, train=False, transform=transform)\n","    train_data = torch.utils.data.DataLoader(\n","        train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(\n","        val_set, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class LeNet(nn.Sequential):\n","  def __init__(self, rpu_config, num_classes, weight_scaling_omega) -> None:\n","    super().__init__()\n","    self.features = nn.Sequential(\n","        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1),\n","        nn.Tanh(),\n","        nn.MaxPool2d(kernel_size=2),\n","        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1),\n","        nn.Tanh(),\n","        nn.MaxPool2d(kernel_size=2),\n","        nn.Tanh() ).cuda()\n","    self.classification = nn.Sequential(\n","    nn.Linear(in_features=512, out_features=128),\n","    nn.Tanh(),\n","    nn.Linear(in_features=128, out_features=N_CLASSES),\n","    ).cuda()\n","    # self.features, self.classification = create_analog_lenet_network(rpu_config, num_classes, weight_scaling_omega)\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = torch.flatten(x, 1)\n","    logits = self.classification(x)\n","    predict = F.softmax(logits, dim=1)\n","    return predict\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        # print(accuracy)\n","        scheduler.step(valid_loss)\n","\n","    # # Save results and plot figures\n","    np.savetxt(os.path.join(RESULTS, \"Test_error\" + ADD_PATH), test_error, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Train_Losses\"+ ADD_PATH), train_losses, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Valid_Losses\" +  ADD_PATH), valid_losses, delimiter=\",\")\n","    plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_sc_test_losses_NO_12Epoch_T1.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_sc_test_error_NO_12Epoch_T1.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('LENET MNIST Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_sc_test_accuracy_NO_12Epoch_T1.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('LENET Aihwkit MNIST Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('LENET Aihwkit MNIST Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('LENET Aihwkit MNIST Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'lenet_mnist_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # create results director for simulation\n","    create_results_dir()\n","\n","    # set seed\n","    # torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_MNIST()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = LeNet(RPU_CONFIG, N_CLASSES, WEIGHT_SCALING_OMEGA)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    # summary(model, (1, 28+28, 512), BATCH_SIZE)\n","    # exit(0)\n","    print(model)\n","\n","    # optimizer\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)\n","    # optimizer.regroup_param_groups(model)\n","\n","    # super convergence scheduler\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.94)\n","    # CyclicLR(optimizer, \n","    #                                           base_lr = 0.01, \n","    #                                           max_lr= 0.1, \n","    #                                           step_size_up= math.floor(N_EPOCHS/2), \n","    #                                           cycle_momentum=True, \n","    #                                           base_momentum=0.95, \n","    #                                           max_momentum=0.85)\n","    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose = True)\n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1)\n","    # StepLR(optimizer, step_size=10, gamma=0.5)\n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1)\n","    #lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=9, verbose = True, threshold=0.0001, threshold_mode='abs', cooldown=1)\n","    #MultiStepLR(optimizer, milestones=[40,45], gamma=0.1) #StepLR(optimizer, step_size=17, gamma=0.1)\n","    \n","\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Lenet Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_sc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS, scheduler)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Lenet Example')\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit/examples/lenet_nsc_aihwkit_mnist.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ye2yUmjCiPXB"},"source":["### Run Code - normal training"]},{"cell_type":"code","metadata":{"id":"JbPeI7xhhwUy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618853543426,"user_tz":-180,"elapsed":224808,"user":{"displayName":"Amna Sajjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgM06eCwjqruN0wUfda-PTPlTbHTk3jGwGxRrY3Pg=s64","userId":"03051534757007455985"}},"outputId":"c5c033f3-62c0-4ac0-e42f-7f1183f7445d"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/lenet_nsc_aihwkit_mnist.py"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/aihw_venv\n","/content/gdrive/MyDrive/aihw_venv/aihwkit_env/aihwkit\n","finished making dir...\n","LeNet(\n","  (features): Sequential(\n","    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n","    (1): Tanh()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n","    (4): Tanh()\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Tanh()\n","  )\n","  (classification): Sequential(\n","    (0): Linear(in_features=512, out_features=128, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=128, out_features=10, bias=True)\n","  )\n",")\n","\n","17:28:43 --- Started Lenet Example\n","17:28:47 --- Epoch: 0\tTrain loss: 2.3028\tValid loss: 2.3025\tTraining accuracy: 9.82%\tTest error: 90.21%\tTest accuracy: 9.79%\t\n","17:28:51 --- Epoch: 1\tTrain loss: 2.3022\tValid loss: 2.3019\tTraining accuracy: 9.86%\tTest error: 90.19%\tTest accuracy: 9.81%\t\n","17:28:56 --- Epoch: 2\tTrain loss: 2.3016\tValid loss: 2.3014\tTraining accuracy: 9.88%\tTest error: 90.19%\tTest accuracy: 9.81%\t\n","17:29:00 --- Epoch: 3\tTrain loss: 2.3010\tValid loss: 2.3007\tTraining accuracy: 9.88%\tTest error: 90.18%\tTest accuracy: 9.82%\t\n","17:29:04 --- Epoch: 4\tTrain loss: 2.3004\tValid loss: 2.3001\tTraining accuracy: 9.92%\tTest error: 90.17%\tTest accuracy: 9.83%\t\n","17:29:09 --- Epoch: 5\tTrain loss: 2.2997\tValid loss: 2.2994\tTraining accuracy: 10.00%\tTest error: 90.07%\tTest accuracy: 9.93%\t\n","17:29:13 --- Epoch: 6\tTrain loss: 2.2990\tValid loss: 2.2986\tTraining accuracy: 10.16%\tTest error: 89.91%\tTest accuracy: 10.09%\t\n","17:29:18 --- Epoch: 7\tTrain loss: 2.2982\tValid loss: 2.2978\tTraining accuracy: 10.44%\tTest error: 89.44%\tTest accuracy: 10.56%\t\n","17:29:22 --- Epoch: 8\tTrain loss: 2.2974\tValid loss: 2.2968\tTraining accuracy: 11.96%\tTest error: 86.37%\tTest accuracy: 13.63%\t\n","17:29:26 --- Epoch: 9\tTrain loss: 2.2964\tValid loss: 2.2958\tTraining accuracy: 15.41%\tTest error: 82.70%\tTest accuracy: 17.30%\t\n","17:29:31 --- Epoch: 10\tTrain loss: 2.2953\tValid loss: 2.2945\tTraining accuracy: 18.71%\tTest error: 80.50%\tTest accuracy: 19.50%\t\n","17:29:35 --- Epoch: 11\tTrain loss: 2.2940\tValid loss: 2.2931\tTraining accuracy: 20.71%\tTest error: 78.86%\tTest accuracy: 21.14%\t\n","17:29:39 --- Epoch: 12\tTrain loss: 2.2924\tValid loss: 2.2914\tTraining accuracy: 21.88%\tTest error: 77.71%\tTest accuracy: 22.29%\t\n","17:29:44 --- Epoch: 13\tTrain loss: 2.2906\tValid loss: 2.2893\tTraining accuracy: 23.00%\tTest error: 76.94%\tTest accuracy: 23.06%\t\n","17:29:48 --- Epoch: 14\tTrain loss: 2.2883\tValid loss: 2.2868\tTraining accuracy: 23.71%\tTest error: 76.23%\tTest accuracy: 23.77%\t\n","17:29:52 --- Epoch: 15\tTrain loss: 2.2854\tValid loss: 2.2834\tTraining accuracy: 24.18%\tTest error: 75.97%\tTest accuracy: 24.03%\t\n","17:29:57 --- Epoch: 16\tTrain loss: 2.2815\tValid loss: 2.2788\tTraining accuracy: 24.32%\tTest error: 75.89%\tTest accuracy: 24.11%\t\n","17:30:01 --- Epoch: 17\tTrain loss: 2.2762\tValid loss: 2.2723\tTraining accuracy: 24.02%\tTest error: 76.14%\tTest accuracy: 23.86%\t\n","17:30:06 --- Epoch: 18\tTrain loss: 2.2684\tValid loss: 2.2625\tTraining accuracy: 23.74%\tTest error: 76.62%\tTest accuracy: 23.38%\t\n","17:30:10 --- Epoch: 19\tTrain loss: 2.2563\tValid loss: 2.2474\tTraining accuracy: 23.18%\tTest error: 76.89%\tTest accuracy: 23.11%\t\n","17:30:14 --- Epoch: 20\tTrain loss: 2.2381\tValid loss: 2.2252\tTraining accuracy: 22.75%\tTest error: 77.15%\tTest accuracy: 22.85%\t\n","17:30:19 --- Epoch: 21\tTrain loss: 2.2143\tValid loss: 2.2000\tTraining accuracy: 22.64%\tTest error: 76.88%\tTest accuracy: 23.12%\t\n","17:30:23 --- Epoch: 22\tTrain loss: 2.1916\tValid loss: 2.1791\tTraining accuracy: 23.47%\tTest error: 75.07%\tTest accuracy: 24.93%\t\n","17:30:27 --- Epoch: 23\tTrain loss: 2.1733\tValid loss: 2.1619\tTraining accuracy: 25.86%\tTest error: 71.96%\tTest accuracy: 28.04%\t\n","17:30:32 --- Epoch: 24\tTrain loss: 2.1573\tValid loss: 2.1459\tTraining accuracy: 29.94%\tTest error: 67.72%\tTest accuracy: 32.28%\t\n","17:30:36 --- Epoch: 25\tTrain loss: 2.1413\tValid loss: 2.1293\tTraining accuracy: 34.09%\tTest error: 63.74%\tTest accuracy: 36.26%\t\n","17:30:40 --- Epoch: 26\tTrain loss: 2.1242\tValid loss: 2.1112\tTraining accuracy: 37.26%\tTest error: 61.49%\tTest accuracy: 38.51%\t\n","17:30:45 --- Epoch: 27\tTrain loss: 2.1050\tValid loss: 2.0912\tTraining accuracy: 39.37%\tTest error: 59.90%\tTest accuracy: 40.10%\t\n","17:30:50 --- Epoch: 28\tTrain loss: 2.0841\tValid loss: 2.0703\tTraining accuracy: 40.77%\tTest error: 58.39%\tTest accuracy: 41.61%\t\n","17:30:54 --- Epoch: 29\tTrain loss: 2.0630\tValid loss: 2.0501\tTraining accuracy: 42.51%\tTest error: 56.60%\tTest accuracy: 43.40%\t\n","17:30:58 --- Epoch: 30\tTrain loss: 2.0428\tValid loss: 2.0306\tTraining accuracy: 45.10%\tTest error: 53.34%\tTest accuracy: 46.66%\t\n","17:31:02 --- Epoch: 31\tTrain loss: 2.0231\tValid loss: 2.0108\tTraining accuracy: 48.58%\tTest error: 49.34%\tTest accuracy: 50.66%\t\n","17:31:07 --- Epoch: 32\tTrain loss: 2.0029\tValid loss: 1.9903\tTraining accuracy: 52.30%\tTest error: 45.91%\tTest accuracy: 54.09%\t\n","17:31:11 --- Epoch: 33\tTrain loss: 1.9813\tValid loss: 1.9676\tTraining accuracy: 55.52%\tTest error: 43.26%\tTest accuracy: 56.74%\t\n","17:31:16 --- Epoch: 34\tTrain loss: 1.9586\tValid loss: 1.9450\tTraining accuracy: 57.77%\tTest error: 41.49%\tTest accuracy: 58.51%\t\n","17:31:20 --- Epoch: 35\tTrain loss: 1.9363\tValid loss: 1.9237\tTraining accuracy: 59.47%\tTest error: 40.00%\tTest accuracy: 60.00%\t\n","17:31:25 --- Epoch: 36\tTrain loss: 1.9154\tValid loss: 1.9037\tTraining accuracy: 61.14%\tTest error: 37.95%\tTest accuracy: 62.05%\t\n","17:31:29 --- Epoch: 37\tTrain loss: 1.8956\tValid loss: 1.8844\tTraining accuracy: 63.09%\tTest error: 35.70%\tTest accuracy: 64.30%\t\n","17:31:34 --- Epoch: 38\tTrain loss: 1.8761\tValid loss: 1.8645\tTraining accuracy: 65.43%\tTest error: 33.21%\tTest accuracy: 66.79%\t\n","17:31:38 --- Epoch: 39\tTrain loss: 1.8560\tValid loss: 1.8434\tTraining accuracy: 68.01%\tTest error: 30.83%\tTest accuracy: 69.17%\t\n","17:31:42 --- Epoch: 40\tTrain loss: 1.8351\tValid loss: 1.8220\tTraining accuracy: 70.53%\tTest error: 27.87%\tTest accuracy: 72.13%\t\n","17:31:46 --- Epoch: 41\tTrain loss: 1.8150\tValid loss: 1.8020\tTraining accuracy: 73.12%\tTest error: 25.21%\tTest accuracy: 74.79%\t\n","17:31:51 --- Epoch: 42\tTrain loss: 1.7968\tValid loss: 1.7845\tTraining accuracy: 75.38%\tTest error: 23.04%\tTest accuracy: 76.96%\t\n","17:31:55 --- Epoch: 43\tTrain loss: 1.7810\tValid loss: 1.7694\tTraining accuracy: 76.83%\tTest error: 21.73%\tTest accuracy: 78.27%\t\n","17:32:00 --- Epoch: 44\tTrain loss: 1.7673\tValid loss: 1.7562\tTraining accuracy: 77.83%\tTest error: 20.98%\tTest accuracy: 79.02%\t\n","17:32:04 --- Epoch: 45\tTrain loss: 1.7553\tValid loss: 1.7446\tTraining accuracy: 78.52%\tTest error: 20.31%\tTest accuracy: 79.69%\t\n","17:32:09 --- Epoch: 46\tTrain loss: 1.7447\tValid loss: 1.7342\tTraining accuracy: 78.98%\tTest error: 19.75%\tTest accuracy: 80.25%\t\n","17:32:13 --- Epoch: 47\tTrain loss: 1.7351\tValid loss: 1.7248\tTraining accuracy: 79.47%\tTest error: 19.27%\tTest accuracy: 80.73%\t\n","17:32:17 --- Epoch: 48\tTrain loss: 1.7263\tValid loss: 1.7161\tTraining accuracy: 79.93%\tTest error: 18.89%\tTest accuracy: 81.11%\t\n","17:32:22 --- Epoch: 49\tTrain loss: 1.7181\tValid loss: 1.7079\tTraining accuracy: 80.35%\tTest error: 18.46%\tTest accuracy: 81.54%\t\n","17:32:22 --- Completed Lenet Example\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BShAcAC8D0QY","executionInfo":{"elapsed":1238,"status":"ok","timestamp":1618492925296,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"c39a71d7-a463-4e9a-e730-242ec1dfe93d"},"source":["#  %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_svhn.py\n","\n","# import os\n","# from datetime import datetime\n","\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","# import math\n","\n","# # Imports from PyTorch.\n","# import torch\n","# import torch.nn.functional as F\n","# from torch import nn\n","# from torchvision import datasets, transforms\n","\n","# # Imports from aihwkit.\n","# from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","# from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","# from aihwkit.optim import AnalogSGD\n","# from aihwkit.simulator.configs import InferenceRPUConfig\n","# from aihwkit.simulator.configs.utils import BoundManagementType, WeightNoiseType\n","# from aihwkit.simulator.noise_models import PCMLikeNoiseModel\n","\n","# # Check device\n","# USE_CUDA = 0\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# if torch.cuda.is_available():\n","#     USE_CUDA = 1\n","\n","\n","# # Path to store datasets\n","# PATH_DATASET = os.path.join('data', 'DATASET')\n","\n","# # Path to store results\n","# RESULTS = os.path.join(os.getcwd(), 'results', 'LENET5')\n","\n","# # Define the properties of the neural network in terms of noise simulated during\n","# # the inference/training pass\n","# RPU_CONFIG = InferenceRPUConfig()\n","# RPU_CONFIG.backward.bound_management = BoundManagementType.NONE\n","# RPU_CONFIG.forward.out_res = -1.  # Turn off (output) ADC discretization.\n","# RPU_CONFIG.forward.w_noise_type = WeightNoiseType.ADDITIVE_CONSTANT\n","# RPU_CONFIG.forward.w_noise = 0.02\n","# RPU_CONFIG.noise_model = PCMLikeNoiseModel(g_max=25.0)\n","\n","# #-----\n","\n","# # ***NOTE*** change to your own path before running\n","# # set paths for training and validation data and results\n","# PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_data\"\n","# # PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_data\"\n","# RESULTS = \"/content/gdrive/MyDrive/ECSE552final/lenet_mnist_aihwkit_results\"\n","# # RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/lenet_mnist_aihwkit_results\"\n","\n","\n","# # Path to store results\n","# # RESULTS = os.path.join(os.getcwd(), 'results', 'LeNet with CIFAR10 dataset')\n","# # Path where the datasets will be stored.\n","# TRAIN_DATASET = 'data/TRAIN_DATASET'\n","# TEST_DATASET = 'data/TEST_DATASET'\n","# PATH_DATASET = os.path.join('data', 'DATASET')\n","\n","# # Training parameters\n","# SEED = 1\n","# N_EPOCHS = 25\n","# BATCH_SIZE = 128\n","# LEARNING_RATE = 0.01\n","# N_CLASSES = 10\n","\n","\n","# RPU_CONFIG = IdealizedPreset()\n","# WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","\n","# # baseline noise setting\n","# # Create RPU configuration with noise\n","# # Noise of the weight computations\n","# # RPU_CONFIG = SingleRPUConfig(device=ExpStepDevice())\n","# # RPU_CONFIG.forward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# # RPU_CONFIG.forward.w_noise = 0.02\n","# # RPU_CONFIG.backward.w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT\n","# # RPU_CONFIG.backward.w_noise = 0.02\n","\n","\n","# def load_images_MNIST():\n","#     \"\"\"Load images for train from the torchvision datasets.\"\"\"\n","#     transform = transforms.Compose([transforms.ToTensor()])\n","\n","#     # Load the images.\n","#     train_set = datasets.MNIST(TRAIN_DATASET,\n","#                                download=True, train=True, transform=transform)\n","#     val_set = datasets.MNIST(TEST_DATASET,\n","#                              download=True, train=False, transform=transform)\n","#     train_data = torch.utils.data.DataLoader(\n","#         train_set, batch_size=BATCH_SIZE, shuffle=True)\n","#     validation_data = torch.utils.data.DataLoader(\n","#         val_set, batch_size=BATCH_SIZE, shuffle=True)\n","\n","#     return train_data, validation_data\n","\n","\n","# def create_analog_lenet_network(rpu_config, num_classes, weight_scaling_omega):\n","#     \"\"\"Create the neural network using analog and digital layers.\n","\n","#     Args:\n","#         rpu_config (int): Type of RPU Config used\n","#         weight_scaling_omega (int): Omega used in the Analog Model\n","#         num_classes (int): How many classes, used for nn.Linear output.\n","#     \"\"\"\n","#     # Include RPU configuration model\n","#     features = AnalogSequential(\n","#         # AnalogConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0,\n","#         #                           rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         # nn.AvgPool2d(out_channels),\n","#         # nn.BatchNorm2d(out_channels),\n","#         # nn.AvgPool2d(),\n","#         # nn.Flatten(),\n","#         # AnalogLinear(hidden_sizes[0], 120, True, rpu_config),\n","#         # AnalogLinear(120, 84, True, rpu_config),\n","#         # nn.Softmax(dim=1)\n","#         AnalogConv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1,\n","#                       rpu_config=rpu_config, weight_scaling_omega=weight_scaling_omega),\n","#         nn.Tanh(),\n","#         nn.MaxPool2d(kernel_size=2),\n","#         AnalogConv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1,\n","#                       rpu_config=rpu_config),\n","#         nn.Tanh(),\n","#         nn.MaxPool2d(kernel_size=2),\n","#         nn.Tanh()\n","#     )\n","\n","#     classification = AnalogSequential(\n","#         AnalogLinear(in_features=512, out_features=128, rpu_config=rpu_config),\n","#         nn.Tanh(),\n","#         AnalogLinear(in_features=128, out_features=num_classes, rpu_config=rpu_config),\n","#     )\n","\n","#     print(features)\n","#     return features, classification\n","\n","# def create_sgd_optimizer(model, learning_rate):\n","#     \"\"\"\n","#     Args:\n","#         model (nn.Module): model to be trained\n","#         learning_rate (float): global parameter to define learning rate\n","#     \"\"\"\n","#     optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","#     optimizer.regroup_param_groups(model)\n","\n","#     return optimizer\n","# # train function\n","# def train(train_data, model, criterion, optimizer):\n","#     \"\"\"Train network.\n","#     Args:\n","#         train_data (DataLoader): Validation set to perform the evaluation\n","#         model (nn.Module): Trained model to be evaluated\n","#         criterion (nn.CrossEntropyLoss): criterion to compute loss\n","#         optimizer (Optimizer): analog model optimizer\n","#     \"\"\"\n","#     total_loss = 0\n","#     predicted_ok = 0\n","#     total_images = 0\n","#     model.train()\n","#     for images, labels in train_data:\n","\n","#         images = images.to(device)\n","#         labels = labels.to(device)\n","#         optimizer.zero_grad()\n","\n","#         # Add training Tensor to the model (input).\n","#         output = model(images)\n","#         loss = criterion(output, labels)\n","#         total_loss += loss.item() * images.size(0)\n","\n","#         _, predicted = torch.max(output.data, 1)\n","#         total_images += labels.size(0)\n","#         predicted_ok += (predicted == labels).sum().item()\n","#         accuracy = predicted_ok/total_images*100\n","\n","#         # Run training (backward propagation).\n","#         loss.backward()\n","\n","#         # Optimize weights.\n","#         optimizer.step()\n","        \n","#     epoch_loss = total_loss / len(train_data.dataset)\n","\n","#     return model, accuracy, optimizer, epoch_loss\n","\n","\n","# # validate function\n","# def validate(validation_data, model, criterion):\n","#     \"\"\"Test trained network\n","#     Args:\n","#         validation_data (DataLoader): Validation set to perform the evaluation\n","#         model (nn.Module): Trained model to be evaluated\n","#         criterion (nn.CrossEntropyLoss): criterion to compute loss\n","#     \"\"\"\n","#     total_loss = 0\n","#     predicted_ok = 0\n","#     total_images = 0\n","\n","#     model.eval()\n","\n","#     for images, labels in validation_data:\n","#         images = images.to(device)\n","#         labels = labels.to(device)\n","\n","#         pred = model(images)\n","#         loss = criterion(pred, labels)\n","#         total_loss += loss.item() * images.size(0)\n","\n","#         _, predicted = torch.max(pred.data, 1)\n","#         total_images += labels.size(0)\n","#         predicted_ok += (predicted == labels).sum().item()\n","#         accuracy = predicted_ok/total_images*100\n","#         error = (1-predicted_ok/total_images)*100\n","\n","#     epoch_loss = total_loss / len(validation_data.dataset)\n","\n","#     return model, epoch_loss, error, accuracy\n","\n","# # train loop for super convergence\n","# def train_val_loop_sc(model, criterion, optimizer, \n","#                    train_data, validation_data, epochs, \n","#                    scheduler, print_every=1):\n","#     \"\"\"Training loop.\n","#     Args:\n","#         model (nn.Module): Trained model to be evaluated\n","#         criterion (nn.CrossEntropyLoss): criterion to compute loss\n","#         optimizer (Optimizer): analog model optimizer\n","#         train_data (DataLoader): Validation set to perform the evaluation\n","#         validation_data (DataLoader): Validation set to perform the evaluation\n","#         epochs (int): global parameter to define epochs number\n","#         print_every (int): defines how many times to print training progress\n","#     \"\"\"\n","#     train_losses = []\n","#     valid_losses = []\n","#     test_error = []\n","#     testing_acc = []\n","\n","#     # Train model\n","#     for epoch in range(0, epochs):\n","        \n","#         # Train_step\n","#         model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","#         train_losses.append(train_loss)\n","\n","#         if epoch % print_every == (print_every - 1):\n","#             # Validate_step\n","#             with torch.no_grad():\n","#                 model, valid_loss, error, accuracy = validate(\n","#                     validation_data, model, criterion)\n","#                 valid_losses.append(valid_loss)\n","#                 test_error.append(error)\n","#                 testing_acc.append(accuracy)\n","\n","\n","#             print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","#                   f'Epoch: {epoch}\\t'\n","#                   f'Train loss: {train_loss:.4f}\\t'\n","#                   f'Valid loss: {valid_loss:.4f}\\t'\n","#                   f'Training accuracy: {training_acc:.2f}%\\t'\n","#                   f'Test error: {error:.2f}%\\t'\n","#                   f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","#         # for cyclic learning rate training \n","#         scheduler.step()\n","\n","#     # Save results and plot figures\n","#     # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","#     # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","#     # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","#     plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","#     return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# # train loop for non super convergence\n","# def train_val_loop_nsc(model, criterion, optimizer, \n","#                    train_data, validation_data, epochs, \n","#                    scheduler, print_every=1):\n","#     \"\"\"Training loop.\n","#     Args:\n","#         model (nn.Module): Trained model to be evaluated\n","#         criterion (nn.CrossEntropyLoss): criterion to compute loss\n","#         optimizer (Optimizer): analog model optimizer\n","#         train_data (DataLoader): Validation set to perform the evaluation\n","#         validation_data (DataLoader): Validation set to perform the evaluation\n","#         epochs (int): global parameter to define epochs number\n","#         print_every (int): defines how many times to print training progress\n","#     \"\"\"\n","#     train_losses = []\n","#     valid_losses = []\n","#     test_error = []\n","#     testing_acc = []\n","\n","#     # Train model\n","#     for epoch in range(0, epochs):\n","        \n","#         # Train_step\n","#         model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","#         train_losses.append(train_loss)\n","\n","#         if epoch % print_every == (print_every - 1):\n","#             # Validate_step\n","#             with torch.no_grad():\n","#                 model, valid_loss, error, accuracy = validate(\n","#                     validation_data, model, criterion)\n","#                 valid_losses.append(valid_loss)\n","#                 test_error.append(error)\n","#                 testing_acc.append(accuracy)\n","\n","\n","#             print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","#                   f'Epoch: {epoch}\\t'\n","#                   f'Train loss: {train_loss:.4f}\\t'\n","#                   f'Valid loss: {valid_loss:.4f}\\t'\n","#                   f'Training accuracy: {training_acc:.2f}%\\t'\n","#                   f'Test error: {error:.2f}%\\t'\n","#                   f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","#     # Save results and plot figures\n","#     # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","#     # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","#     # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","#     plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","#     return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# # plots sc\n","# def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","#     \"\"\"Plot results.\n","#     Args:\n","#         train_losses: training losses as calculated in the training_loop\n","#         valid_losses: validation losses as calculated in the training_loop\n","#         test_error: test error as calculated in the training_loop\n","#     \"\"\"\n","#     fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","#     plt.title('LeNet5 SVHN Aihwkit Super Convergence Loss')\n","#     plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Loss [A.U.]')\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_sc_test_losses.png'))\n","#     plt.close()\n","\n","#     fig = plt.plot(test_error, 'r-s')\n","#     plt.title('LeNet5 SVHN Aihwkit Super Convergence Test Error')\n","#     plt.legend(fig[:1], ['Test Error'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Test Error [%]')\n","#     plt.ylim((0, 1e2))\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_sc_test_error.png'))\n","#     plt.close()\n","\n","#     fig = plt.plot(test_acc, 'r-s')\n","#     plt.title('LeNet5 SVHN Aihwkit Super Convergence Accuracy')\n","#     plt.legend(fig[:1], ['Test accuracy'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Test accuracy [%]')\n","#     plt.ylim((0, 1e2))\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_sc_test_accuracy.png'))\n","#     plt.close()\n","\n","# def plot_results(train_losses, valid_losses, test_error, test_acc):\n","#     \"\"\"Plot results.\n","#     Args:\n","#         train_losses: training losses as calculated in the training_loop\n","#         valid_losses: validation losses as calculated in the training_loop\n","#         test_error: test error as calculated in the training_loop\n","#     \"\"\"\n","#     fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","#     plt.title('LeNet5 Aihwkit SVHN Loss')\n","#     plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Loss [A.U.]')\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_nsc_test_losses.png'))\n","#     plt.close()\n","\n","#     fig = plt.plot(test_error, 'r-s')\n","#     plt.title('Lenet5 Aihwkit SVHN Test Error')\n","#     plt.legend(fig[:1], ['Test Error'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Test Error [%]')\n","#     plt.ylim((0, 1e2))\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_nsc_test_error.png'))\n","#     plt.close()\n","\n","#     fig = plt.plot(test_acc, 'r-s')\n","#     plt.title('LeNet5 Aihwkit SVHN Accuracy')\n","#     plt.legend(fig[:1], ['Test accuracy'])\n","#     plt.xlabel('Epoch number')\n","#     plt.ylabel('Test accuracy [%]')\n","#     plt.ylim((0, 1e2))\n","#     plt.grid(which='both', linestyle='--')\n","#     plt.savefig(os.path.join(RESULTS, 'lenet5_svhn_aihwkit_nsc_test_accuracy.png'))\n","#     plt.close()\n","\n","# class LeNet(nn.Module):\n","#   def __init__(self, rpu_config, num_classes, weight_scaling_omega) -> None:\n","#     super().__init__()\n","#     features = AnalogSequential(\n","#         AnalogConv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1,\n","#                       rpu_config=rpu_config, weight_scaling_omega=weight_scaling_omega),\n","#         nn.Tanh(),\n","#         nn.MaxPool2d(kernel_size=2),\n","#         AnalogConv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1,\n","#                       rpu_config=rpu_config),\n","#         nn.Tanh(),\n","#         nn.MaxPool2d(kernel_size=2),\n","#         nn.Tanh() ).cuda()\n","#     # self.features, self.classification = create_analog_lenet_network(rpu_config, num_classes, weight_scaling_omega)\n","#   def forward(self, x):\n","#     x = self.features(x)\n","#     x = torch.flatten(x, 1)\n","#     logits = self.classification(x)\n","#     predict = F.softmax(logits, dim=1)\n","#     return predict\n","\n","# if __name__ == '__main__':\n","\n","#     # create results director for simulation\n","#     # create_results_dir()\n","\n","#     # set seed\n","#     torch.manual_seed(SEED)\n","\n","#     # Load datasets.\n","#     train_data, validation_data = load_images_MNIST()\n","\n","#     # Prepare the model.\n","#     model = LeNet(RPU_CONFIG, N_CLASSES, WEIGHT_SCALING_OMEGA)\n","#     if USE_CUDA:\n","#         model.cuda()\n","#     print(model)\n","\n","#     # optimizer\n","#     # optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n","#     # optimizer.regroup_param_groups(model)\n","#     optimizer = create_sgd_optimizer(model, LEARNING_RATE)\n","\n","#     # super convergence scheduler\n","#     scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n","#                                               base_lr = 0.001, \n","#                                               max_lr= 0.1, \n","#                                               step_size_up= math.floor(N_EPOCHS/2), \n","#                                               cycle_momentum=True, \n","#                                               base_momentum=0.95, \n","#                                               max_momentum=0.8)\n","#     # loss function\n","#     criterion = nn.CrossEntropyLoss()\n","\n","\n","#     print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","#       f'Started LetNet5 Example')\n","#     # training loop\n","#     model, optimizer, _ = train_val_loop_sc(model, criterion, optimizer, \n","#                                     train_data, validation_data,\n","#                                         N_EPOCHS, scheduler)\n","#     print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","#       f'Completed LetNet5 Example')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/lenet_sc_aihwkit_svhn.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"QMML4cCQ9h3d","executionInfo":{"elapsed":1526,"status":"error","timestamp":1618491390845,"user":{"displayName":"Mian Hamza","photoUrl":"","userId":"12004810550252796141"},"user_tz":-180},"outputId":"21ab49f4-2fb5-4c8a-8281-f0d764355480"},"source":["for epoch in range(N_EPOCHS):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(train_dl, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        # print(labels)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        \n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 50 == 49:    # print every 50 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 50))\n","            running_loss = 0.0\n","\n","print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-bb5f7fa85a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Ok-4WkWa8KX2"},"source":["## SVHN - VGG8"]},{"cell_type":"markdown","metadata":{"id":"HBp4ktXk9gVI"},"source":["### Write file - super convergence"]},{"cell_type":"code","metadata":{"id":"we_wBau38NVQ"},"source":[" %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_sc_aihwkit_svhn.py\n","# %%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_sc_aihwkit_svhn.py\n","\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","SEED = 1\n","N_EPOCHS = 25\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05%\tTest accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# ***NOTE*** change to your own path before running\n","# set paths for training and validation data and results\n","# PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_data\"\n","PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_data\"\n","# RESULTS = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_aihwkit_results\"\n","RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_aihwkit_results\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","   # path = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_aihwkit_results\"\n","  path = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","\n","# load data\n","def load_images_svhn():\n","    \"\"\"Load images for train from torchvision datasets.\"\"\"\n","\n","    mean = torch.tensor([0.4377, 0.4438, 0.4728])\n","    std = torch.tensor([0.1980, 0.2010, 0.1970])\n","\n","    print(f'Normalization data: ({mean},{std})')\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","    train_set = datasets.SVHN(PATH_DATASET, download=True, split='train', transform=transform)\n","    val_set = datasets.SVHN(PATH_DATASET, download=True, split='test', transform=transform)\n","    train_set.data = train_set.data[0:50000]\n","    val_set.data = val_set.data[0:10000]\n","    print(len(train_set), len(val_set))\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, \n","                                             shuffle=True, drop_last=True, \n","                                             pin_memory=True, num_workers=2)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, \n","                                                  shuffle=False, drop_last=True, \n","                                                  pin_memory=True, num_workers=2)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class VGG8(nn.Module):\n","  def __init__(self, in_channels, num_classes):\n","    super(VGG8, self).__init__()\n","    self.layers = AnalogSequential(\n","        AnalogConv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(512),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        nn.Flatten(),\n","        AnalogLinear(in_features=8192, out_features=1024,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogLinear(in_features=1024, out_features=num_classes,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.LogSoftmax(dim=1)\n","    ).cuda()\n","\n","  def forward(self, data):\n","    return self.layers(data)\n","\n","\n","# architecture\n","# def VGG8():\n","#     \"\"\"VGG8 inspired analog model.\"\"\"\n","#     model = AnalogSequential(\n","#         AnalogConv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(128),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(256),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(512),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         nn.Flatten(),\n","#         AnalogLinear(in_features=8192, out_features=1024,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogLinear(in_features=1024, out_features=N_CLASSES,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.LogSoftmax(dim=1)\n","#     ).cuda()\n","\n","#     return model\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        scheduler.step()\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_accuracy.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Aihwkit SVHN Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Aihwkit SVHN Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Aihwkit SVHN Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # create results director for simulation\n","    create_results_dir()\n","\n","    # set seed\n","    torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_svhn()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = VGG8(in_channels=3, num_classes=N_CLASSES)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    summary(model, (3, 32, 32), BATCH_SIZE)\n","    # exit(0)\n","    # print(model)\n","\n","    # optimizer\n","    optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n","    optimizer.regroup_param_groups(model)\n","\n","    # super convergence scheduler\n","    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n","                                              base_lr = 0.01, \n","                                              max_lr= 0.2, \n","                                              step_size_up= math.floor(N_EPOCHS/2), \n","                                              cycle_momentum=True, \n","                                              base_momentum=0.95, \n","                                              max_momentum=0.8)\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Vgg8 Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_sc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS, scheduler)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Vgg8 Example')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usr902sBahVe"},"source":["### Write file - normal training"]},{"cell_type":"code","metadata":{"id":"-ELvboWMav-I"},"source":["#  %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_nsc_aihwkit_svhn.py\n","%%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_nsc_aihwkit_svhn.py\n","\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","SEED = 1\n","N_EPOCHS = 50\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05%\tTest accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# set paths for training and validation data and results\n","# PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_data\"\n","PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_data\"\n","# RESULTS = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_aihwkit_results\"\n","RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_aihwkit_results\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","  # path = \"/content/gdrive/MyDrive/ECSE552final/vgg8_svhn_aihwkit_results\"\n","  path = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","\n","# load data\n","def load_images_svhn():\n","    \"\"\"Load images for train from torchvision datasets.\"\"\"\n","\n","    mean = torch.tensor([0.4377, 0.4438, 0.4728])\n","    std = torch.tensor([0.1980, 0.2010, 0.1970])\n","\n","    print(f'Normalization data: ({mean},{std})')\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","    train_set = datasets.SVHN(PATH_DATASET, download=True, split='train', transform=transform)\n","    val_set = datasets.SVHN(PATH_DATASET, download=True, split='test', transform=transform)\n","    # train_set.data = train_set.data[0:50000]\n","    # val_set.data = val_set.data[0:10000]\n","    print(len(train_set), len(val_set))\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, \n","                                             shuffle=True, drop_last=True, \n","                                             pin_memory=True, num_workers=2)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, \n","                                                  shuffle=False, drop_last=True, \n","                                                  pin_memory=True, num_workers=2)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class VGG8(nn.Module):\n","  def __init__(self, in_channels, num_classes):\n","    super(VGG8, self).__init__()\n","    self.layers = AnalogSequential(\n","        AnalogConv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(512),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        nn.Flatten(),\n","        AnalogLinear(in_features=8192, out_features=1024,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogLinear(in_features=1024, out_features=num_classes,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.LogSoftmax(dim=1)\n","    ).cuda()\n","\n","  def forward(self, data):\n","    return self.layers(data)\n","\n","\n","# architecture\n","# def VGG8():\n","#     \"\"\"VGG8 inspired analog model.\"\"\"\n","#     model = AnalogSequential(\n","#         AnalogConv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(128),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(256),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(512),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         nn.Flatten(),\n","#         AnalogLinear(in_features=8192, out_features=1024,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogLinear(in_features=1024, out_features=N_CLASSES,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.LogSoftmax(dim=1)\n","#     ).cuda()\n","\n","#     return model\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        scheduler.step()\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 SVHN Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_sc_test_accuracy.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Aihwkit SVHN Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Aihwkit SVHN Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Aihwkit SVHN Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_svhn_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # create results director for simulation\n","    # create_results_dir()\n","\n","    # set seed\n","    torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_svhn()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = VGG8(in_channels=3, num_classes=N_CLASSES)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    summary(model, (3, 32, 32), BATCH_SIZE)\n","    # exit(0)\n","    # print(model)\n","\n","    # optimizer\n","    optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n","    optimizer.regroup_param_groups(model)\n","\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Vgg8 Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_nsc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Vgg8 Example')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hp3qdd-S9RyN"},"source":["### Run code - super convergence"]},{"cell_type":"code","metadata":{"id":"yxswIVoKeoU8"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwk ait/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/vgg8_sc_aihwkit_svhn.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u-aiG302bDPT"},"source":["### Run code - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI68i-3tbTlB","executionInfo":{"elapsed":11400142,"status":"ok","timestamp":1618434348563,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"f8815ae1-6f0a-4eb7-bf96-3269eb00d03a"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/vgg8_nsc_aihwkit_svhn.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final\n","/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit\n","Normalization data: (tensor([0.4377, 0.4438, 0.4728]),tensor([0.1980, 0.2010, 0.1970]))\n","Using downloaded and verified file: /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_data/train_32x32.mat\n","Using downloaded and verified file: /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_svhn_data/test_32x32.mat\n","73257 26032\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","      AnalogConv2d-1         [128, 128, 32, 32]           3,584\n","              ReLU-2         [128, 128, 32, 32]               0\n","      AnalogConv2d-3         [128, 128, 32, 32]         147,584\n","       BatchNorm2d-4         [128, 128, 32, 32]             256\n","              ReLU-5         [128, 128, 32, 32]               0\n","         MaxPool2d-6         [128, 128, 16, 16]               0\n","      AnalogConv2d-7         [128, 256, 16, 16]         295,168\n","              ReLU-8         [128, 256, 16, 16]               0\n","      AnalogConv2d-9         [128, 256, 16, 16]         590,080\n","      BatchNorm2d-10         [128, 256, 16, 16]             512\n","             ReLU-11         [128, 256, 16, 16]               0\n","        MaxPool2d-12           [128, 256, 8, 8]               0\n","     AnalogConv2d-13           [128, 512, 8, 8]       1,180,160\n","             ReLU-14           [128, 512, 8, 8]               0\n","     AnalogConv2d-15           [128, 512, 8, 8]       2,359,808\n","      BatchNorm2d-16           [128, 512, 8, 8]           1,024\n","             ReLU-17           [128, 512, 8, 8]               0\n","        MaxPool2d-18           [128, 512, 4, 4]               0\n","          Flatten-19                [128, 8192]               0\n","     AnalogLinear-20                [128, 1024]       8,389,632\n","             ReLU-21                [128, 1024]               0\n","     AnalogLinear-22                  [128, 10]          10,250\n","       LogSoftmax-23                  [128, 10]               0\n","================================================================\n","Total params: 12,978,058\n","Trainable params: 12,978,058\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.50\n","Forward/backward pass size (MB): 1186.02\n","Params size (MB): 49.51\n","Estimated Total Size (MB): 1237.03\n","----------------------------------------------------------------\n","\n","17:56:18 --- Started Vgg8 Example\n","18:00:35 --- Epoch: 0\tTrain loss: 0.8552\tValid loss: 0.4714\tTraining accuracy: 72.71%\tTest error: 13.38%\tTest accuracy: 86.62%\t\n","18:04:39 --- Epoch: 1\tTrain loss: 0.3820\tValid loss: 0.3839\tTraining accuracy: 88.74%\tTest error: 11.34%\tTest accuracy: 88.66%\t\n","18:08:40 --- Epoch: 2\tTrain loss: 0.3021\tValid loss: 0.3283\tTraining accuracy: 91.19%\tTest error: 9.74%\tTest accuracy: 90.26%\t\n","18:12:37 --- Epoch: 3\tTrain loss: 0.2559\tValid loss: 0.3018\tTraining accuracy: 92.61%\tTest error: 8.53%\tTest accuracy: 91.47%\t\n","18:16:34 --- Epoch: 4\tTrain loss: 0.2227\tValid loss: 0.2929\tTraining accuracy: 93.62%\tTest error: 8.57%\tTest accuracy: 91.43%\t\n","18:20:30 --- Epoch: 5\tTrain loss: 0.1975\tValid loss: 0.2663\tTraining accuracy: 94.45%\tTest error: 7.61%\tTest accuracy: 92.39%\t\n","18:24:25 --- Epoch: 6\tTrain loss: 0.1763\tValid loss: 0.2534\tTraining accuracy: 95.10%\tTest error: 7.12%\tTest accuracy: 92.88%\t\n","18:28:18 --- Epoch: 7\tTrain loss: 0.1558\tValid loss: 0.2556\tTraining accuracy: 95.83%\tTest error: 7.17%\tTest accuracy: 92.83%\t\n","18:32:11 --- Epoch: 8\tTrain loss: 0.1372\tValid loss: 0.2444\tTraining accuracy: 96.43%\tTest error: 6.80%\tTest accuracy: 93.20%\t\n","18:36:05 --- Epoch: 9\tTrain loss: 0.1236\tValid loss: 0.2460\tTraining accuracy: 96.79%\tTest error: 7.12%\tTest accuracy: 92.88%\t\n","18:39:57 --- Epoch: 10\tTrain loss: 0.1108\tValid loss: 0.2453\tTraining accuracy: 97.16%\tTest error: 6.88%\tTest accuracy: 93.12%\t\n","18:43:49 --- Epoch: 11\tTrain loss: 0.0954\tValid loss: 0.2349\tTraining accuracy: 97.66%\tTest error: 6.52%\tTest accuracy: 93.48%\t\n","18:47:41 --- Epoch: 12\tTrain loss: 0.0843\tValid loss: 0.2460\tTraining accuracy: 97.98%\tTest error: 6.98%\tTest accuracy: 93.02%\t\n","18:51:31 --- Epoch: 13\tTrain loss: 0.0685\tValid loss: 0.2497\tTraining accuracy: 98.52%\tTest error: 6.80%\tTest accuracy: 93.20%\t\n","18:55:20 --- Epoch: 14\tTrain loss: 0.0577\tValid loss: 0.2353\tTraining accuracy: 98.78%\tTest error: 6.58%\tTest accuracy: 93.42%\t\n","18:59:08 --- Epoch: 15\tTrain loss: 0.0482\tValid loss: 0.2572\tTraining accuracy: 99.05%\tTest error: 7.24%\tTest accuracy: 92.76%\t\n","19:02:55 --- Epoch: 16\tTrain loss: 0.0394\tValid loss: 0.2377\tTraining accuracy: 99.30%\tTest error: 6.54%\tTest accuracy: 93.46%\t\n","19:06:41 --- Epoch: 17\tTrain loss: 0.0318\tValid loss: 0.2377\tTraining accuracy: 99.47%\tTest error: 6.37%\tTest accuracy: 93.63%\t\n","19:10:27 --- Epoch: 18\tTrain loss: 0.0253\tValid loss: 0.2380\tTraining accuracy: 99.64%\tTest error: 6.29%\tTest accuracy: 93.71%\t\n","19:14:12 --- Epoch: 19\tTrain loss: 0.0209\tValid loss: 0.2344\tTraining accuracy: 99.75%\tTest error: 6.24%\tTest accuracy: 93.76%\t\n","19:17:55 --- Epoch: 20\tTrain loss: 0.0164\tValid loss: 0.2451\tTraining accuracy: 99.84%\tTest error: 6.34%\tTest accuracy: 93.66%\t\n","19:21:39 --- Epoch: 21\tTrain loss: 0.0133\tValid loss: 0.2609\tTraining accuracy: 99.90%\tTest error: 6.69%\tTest accuracy: 93.31%\t\n","19:25:22 --- Epoch: 22\tTrain loss: 0.0106\tValid loss: 0.2480\tTraining accuracy: 99.93%\tTest error: 6.27%\tTest accuracy: 93.73%\t\n","19:29:06 --- Epoch: 23\tTrain loss: 0.0086\tValid loss: 0.2534\tTraining accuracy: 99.97%\tTest error: 6.30%\tTest accuracy: 93.70%\t\n","19:32:49 --- Epoch: 24\tTrain loss: 0.0076\tValid loss: 0.2491\tTraining accuracy: 99.98%\tTest error: 5.99%\tTest accuracy: 94.01%\t\n","19:36:32 --- Epoch: 25\tTrain loss: 0.0062\tValid loss: 0.2515\tTraining accuracy: 99.99%\tTest error: 6.27%\tTest accuracy: 93.73%\t\n","19:40:14 --- Epoch: 26\tTrain loss: 0.0056\tValid loss: 0.2566\tTraining accuracy: 99.99%\tTest error: 6.11%\tTest accuracy: 93.89%\t\n","19:43:57 --- Epoch: 27\tTrain loss: 0.0050\tValid loss: 0.2570\tTraining accuracy: 99.99%\tTest error: 6.24%\tTest accuracy: 93.76%\t\n","19:47:38 --- Epoch: 28\tTrain loss: 0.0044\tValid loss: 0.2602\tTraining accuracy: 99.99%\tTest error: 6.32%\tTest accuracy: 93.68%\t\n","19:51:20 --- Epoch: 29\tTrain loss: 0.0041\tValid loss: 0.2582\tTraining accuracy: 99.99%\tTest error: 6.20%\tTest accuracy: 93.80%\t\n","19:55:03 --- Epoch: 30\tTrain loss: 0.0038\tValid loss: 0.2574\tTraining accuracy: 99.99%\tTest error: 6.08%\tTest accuracy: 93.92%\t\n","19:58:47 --- Epoch: 31\tTrain loss: 0.0036\tValid loss: 0.2595\tTraining accuracy: 99.99%\tTest error: 6.21%\tTest accuracy: 93.79%\t\n","20:02:30 --- Epoch: 32\tTrain loss: 0.0032\tValid loss: 0.2591\tTraining accuracy: 100.00%\tTest error: 6.13%\tTest accuracy: 93.87%\t\n","20:06:15 --- Epoch: 33\tTrain loss: 0.0030\tValid loss: 0.2617\tTraining accuracy: 100.00%\tTest error: 6.05%\tTest accuracy: 93.95%\t\n","20:10:00 --- Epoch: 34\tTrain loss: 0.0029\tValid loss: 0.2657\tTraining accuracy: 100.00%\tTest error: 6.19%\tTest accuracy: 93.81%\t\n","20:13:45 --- Epoch: 35\tTrain loss: 0.0027\tValid loss: 0.2621\tTraining accuracy: 100.00%\tTest error: 6.08%\tTest accuracy: 93.92%\t\n","20:17:29 --- Epoch: 36\tTrain loss: 0.0027\tValid loss: 0.2679\tTraining accuracy: 100.00%\tTest error: 6.18%\tTest accuracy: 93.82%\t\n","20:21:14 --- Epoch: 37\tTrain loss: 0.0025\tValid loss: 0.2696\tTraining accuracy: 99.99%\tTest error: 6.17%\tTest accuracy: 93.83%\t\n","20:24:58 --- Epoch: 38\tTrain loss: 0.0024\tValid loss: 0.2697\tTraining accuracy: 99.99%\tTest error: 6.15%\tTest accuracy: 93.85%\t\n","20:28:41 --- Epoch: 39\tTrain loss: 0.0022\tValid loss: 0.2740\tTraining accuracy: 100.00%\tTest error: 6.11%\tTest accuracy: 93.89%\t\n","20:32:23 --- Epoch: 40\tTrain loss: 0.0020\tValid loss: 0.2732\tTraining accuracy: 100.00%\tTest error: 6.33%\tTest accuracy: 93.67%\t\n","20:36:06 --- Epoch: 41\tTrain loss: 0.0021\tValid loss: 0.2722\tTraining accuracy: 99.99%\tTest error: 6.07%\tTest accuracy: 93.93%\t\n","20:39:49 --- Epoch: 42\tTrain loss: 0.0019\tValid loss: 0.2702\tTraining accuracy: 100.00%\tTest error: 5.97%\tTest accuracy: 94.03%\t\n","20:43:31 --- Epoch: 43\tTrain loss: 0.0020\tValid loss: 0.2727\tTraining accuracy: 100.00%\tTest error: 6.10%\tTest accuracy: 93.90%\t\n","20:47:14 --- Epoch: 44\tTrain loss: 0.0019\tValid loss: 0.2758\tTraining accuracy: 99.99%\tTest error: 6.06%\tTest accuracy: 93.94%\t\n","20:50:57 --- Epoch: 45\tTrain loss: 0.0018\tValid loss: 0.2764\tTraining accuracy: 99.99%\tTest error: 6.12%\tTest accuracy: 93.88%\t\n","20:54:40 --- Epoch: 46\tTrain loss: 0.0016\tValid loss: 0.2793\tTraining accuracy: 100.00%\tTest error: 6.13%\tTest accuracy: 93.87%\t\n","20:58:22 --- Epoch: 47\tTrain loss: 0.0017\tValid loss: 0.2755\tTraining accuracy: 100.00%\tTest error: 6.18%\tTest accuracy: 93.82%\t\n","21:02:04 --- Epoch: 48\tTrain loss: 0.0016\tValid loss: 0.2770\tTraining accuracy: 100.00%\tTest error: 6.08%\tTest accuracy: 93.92%\t\n","21:05:46 --- Epoch: 49\tTrain loss: 0.0015\tValid loss: 0.2795\tTraining accuracy: 100.00%\tTest error: 6.17%\tTest accuracy: 93.83%\t\n","21:05:47 --- Completed Vgg8 Example\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1sKT8IfnFme_"},"source":["## CIFAR10 - VGG8"]},{"cell_type":"markdown","metadata":{"id":"Ksp-5-n0FzRC"},"source":["### Write file - super convergence"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUFzj0iCGCaz","executionInfo":{"elapsed":1814,"status":"ok","timestamp":1618340852319,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"dfb8dd54-5efa-4774-e1a0-69e2201f6ca8"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_sc_aihwkit_cifar10.py\n","%%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_sc_aihwkit_cifar10.py\n","\n","\n","\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","SEED = 1\n","N_EPOCHS = 25\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05%\tTest accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# *** NOTE *** make sure to change to your own path after\n","# set paths for training and validation data and results\n","# PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_data\"\n","PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_cifar10_data\"\n","# RESULTS = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","  # path = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","  path = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","\n","# load data\n","def load_images_cifar10():\n","    \"\"\"Load images for train from torchvision datasets.\"\"\"\n","\n","    mean = torch.tensor([0.4914, 0.4822, 0.4465])\n","    std = torch.tensor([0.2023, 0.1994, 0.2010])\n","\n","    print(f'Normalization data: ({mean},{std})')\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","    train_set = datasets.CIFAR10(PATH_DATASET, download=True, train=True, transform=transform)\n","    val_set = datasets.CIFAR10(PATH_DATASET, download=True, train=False, transform=transform)\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class VGG8(nn.Module):\n","  def __init__(self, in_channels, num_classes):\n","    super(VGG8, self).__init__()\n","    self.layers = AnalogSequential(\n","        AnalogConv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(512),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        nn.Flatten(),\n","        AnalogLinear(in_features=8192, out_features=1024,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogLinear(in_features=1024, out_features=num_classes,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.LogSoftmax(dim=1)\n","    ).cuda()\n","\n","  def forward(self, data):\n","    return self.layers(data)\n","\n","\n","# architecture\n","# def VGG8():\n","#     \"\"\"VGG8 inspired analog model.\"\"\"\n","#     model = AnalogSequential(\n","#         AnalogConv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(128),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(256),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(512),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         nn.Flatten(),\n","#         AnalogLinear(in_features=8192, out_features=1024,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogLinear(in_features=1024, out_features=N_CLASSES,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.LogSoftmax(dim=1)\n","#     ).cuda()\n","\n","#     return model\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        scheduler.step()\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_accuracy.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Aihwkit Cifar10 Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Aihwkit Cifar10 Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Aihwkit Cifar10 Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # create results director for simulation\n","    create_results_dir()\n","\n","    # set seed\n","    torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_cifar10()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = VGG8(in_channels=3, num_classes=N_CLASSES)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    summary(model, (3, 32, 32), BATCH_SIZE)\n","    # exit(0)\n","    # print(model)\n","\n","    # optimizer\n","    optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n","    optimizer.regroup_param_groups(model)\n","\n","    # super convergence scheduler\n","    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n","                                              base_lr = 0.01, \n","                                              max_lr= 0.15, \n","                                              step_size_up= math.floor(N_EPOCHS/2), \n","                                              cycle_momentum=True, \n","                                              base_momentum=0.95, \n","                                              max_momentum=0.8)\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Vgg8 Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_sc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS, scheduler)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Vgg8 Example')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_sc_aihwkit_cifar10.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"keUkFBWyT9vL"},"source":["### Write file - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2oGzP9LUHHz","executionInfo":{"elapsed":550,"status":"ok","timestamp":1618436387811,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"f4fb255f-750c-4c09-a663-2546257ace4b"},"source":["# %%writefile /content/gdrive/MyDrive/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_nsc_aihwkit_cifar10.py\n","%%writefile /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_nsc_aihwkit_cifar10.py\n","\n","import os\n","from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import math\n","import numpy as np\n","\n","# Imports from PyTorch.\n","import torch\n","from torchsummary import summary\n","from torch import nn\n","from torchvision import datasets, transforms\n","\n","# Imports from aihwkit.\n","from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n","from aihwkit.optim import AnalogSGD\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset\n","from aihwkit.simulator.presets.configs import GokmenVlasovPreset,IdealizedPreset\n","from aihwkit.simulator.configs import SingleRPUConfig\n","from aihwkit.simulator.configs.devices import ConstantStepDevice\n","from aihwkit.simulator.configs.devices import ExpStepDevice\n","from aihwkit.simulator.configs.utils import (WeightNoiseType, WeightClipType, WeightModifierType)\n","\n","# Check device\n","USE_CUDA = 0\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    USE_CUDA = 1\n","\n","# Training parameters\n","SEED = 1\n","N_EPOCHS = 50\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.01\n","N_CLASSES = 10\n","\n","\n","RPU_CONFIG = IdealizedPreset()\n","WEIGHT_SCALING_OMEGA = 0.8  # the weight value where the max weight will be scaled to. If zero, no weight scaling will be performed.\n","\n","# all with ideal device\n","# Training accuracy: ~60%\tTest accuracy: ~5X% with omega = 0.0\n","# Training accuracy: 81.47%\tTest accuracy: 77.70% with omega = 1\n","# Training accuracy: 82.05%\tTest accuracy: 78.46% with omega = 0.8\n","\n","# WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n","# Select the device model to use in the training. In this case we are using one of the preset,\n","# but it can be changed to a number of preset to explore possible different analog devices\n","# RPU_CONFIG = GokmenVlasovPreset()\n","\n","# *** NOTE *** make sure to change to your own path after\n","# set paths for training and validation data and results\n","# PATH_DATASET = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_data\"\n","PATH_DATASET = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_cifar10_data\"\n","# RESULTS = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","RESULTS = \"/content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","\n","# create directories to store data and results\n","def create_results_dir():\n","  path = \"/content/gdrive/MyDrive/ECSE552final/vgg8_cifar10_aihwkit_results\"\n","  os.makedirs(path, exist_ok=True)\n","  print(\"finished making dir...\")\n","\n","\n","# load data\n","def load_images_cifar10():\n","    \"\"\"Load images for train from torchvision datasets.\"\"\"\n","\n","    mean = torch.tensor([0.4914, 0.4822, 0.4465])\n","    std = torch.tensor([0.2023, 0.1994, 0.2010])\n","\n","    print(f'Normalization data: ({mean},{std})')\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(), transforms.Normalize(mean, std)])\n","    train_set = datasets.CIFAR10(PATH_DATASET, download=True, train=True, transform=transform)\n","    val_set = datasets.CIFAR10(PATH_DATASET, download=True, train=False, transform=transform)\n","    train_data = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","    validation_data = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    return train_data, validation_data\n","\n","#architecture\n","class VGG8(nn.Module):\n","  def __init__(self, in_channels, num_classes):\n","    super(VGG8, self).__init__()\n","    self.layers = AnalogSequential(\n","        AnalogConv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.BatchNorm2d(512),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","        nn.Flatten(),\n","        AnalogLinear(in_features=8192, out_features=1024,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.ReLU(),\n","        AnalogLinear(in_features=1024, out_features=num_classes,\n","                     rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","        nn.LogSoftmax(dim=1)\n","    ).cuda()\n","\n","  def forward(self, data):\n","    return self.layers(data)\n","\n","\n","# architecture\n","# def VGG8():\n","#     \"\"\"VGG8 inspired analog model.\"\"\"\n","#     model = AnalogSequential(\n","#         AnalogConv2d(in_channels=3, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(128),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(256),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         AnalogConv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogConv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.BatchNorm2d(512),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n","#         nn.Flatten(),\n","#         AnalogLinear(in_features=8192, out_features=1024,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.ReLU(),\n","#         AnalogLinear(in_features=1024, out_features=N_CLASSES,\n","#                      rpu_config=RPU_CONFIG, weight_scaling_omega=WEIGHT_SCALING_OMEGA),\n","#         nn.LogSoftmax(dim=1)\n","#     ).cuda()\n","\n","#     return model\n","\n","# optimizer\n","def create_sgd_optimizer(model, learning_rate):\n","    \"\"\"Create the analog-aware optimizer.\n","    Args:\n","        model (nn.Module): model to be trained\n","        learning_rate (float): global parameter to define learning rate\n","    \"\"\"\n","    optimizer = AnalogSGD(model.parameters(), lr=learning_rate)\n","    optimizer.regroup_param_groups(model)\n","\n","    return optimizer\n","\n","# train function\n","def train(train_data, model, criterion, optimizer):\n","    \"\"\"Train network.\n","    Args:\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","    model.train()\n","    for images, labels in train_data:\n","\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Add training Tensor to the model (input).\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","\n","        # Run training (backward propagation).\n","        loss.backward()\n","\n","        # Optimize weights.\n","        optimizer.step()\n","        \n","    epoch_loss = total_loss / len(train_data.dataset)\n","\n","    return model, accuracy, optimizer, epoch_loss\n","\n","\n","# validate function\n","def validate(validation_data, model, criterion):\n","    \"\"\"Test trained network\n","    Args:\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","    \"\"\"\n","    total_loss = 0\n","    predicted_ok = 0\n","    total_images = 0\n","\n","    model.eval()\n","\n","    for images, labels in validation_data:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        pred = model(images)\n","        loss = criterion(pred, labels)\n","        total_loss += loss.item() * images.size(0)\n","\n","        _, predicted = torch.max(pred.data, 1)\n","        total_images += labels.size(0)\n","        predicted_ok += (predicted == labels).sum().item()\n","        accuracy = predicted_ok/total_images*100\n","        error = (1-predicted_ok/total_images)*100\n","\n","    epoch_loss = total_loss / len(validation_data.dataset)\n","\n","    return model, epoch_loss, error, accuracy\n","\n","# train loop for super convergence\n","def train_val_loop_sc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, \n","                   scheduler, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","        \n","        # for cyclic learning rate training \n","        scheduler.step()\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results_super_convergence(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# train loop for non super convergence\n","def train_val_loop_nsc(model, criterion, optimizer, \n","                   train_data, validation_data, epochs, print_every=1):\n","    \"\"\"Training loop.\n","    Args:\n","        model (nn.Module): Trained model to be evaluated\n","        criterion (nn.CrossEntropyLoss): criterion to compute loss\n","        optimizer (Optimizer): analog model optimizer\n","        train_data (DataLoader): Validation set to perform the evaluation\n","        validation_data (DataLoader): Validation set to perform the evaluation\n","        epochs (int): global parameter to define epochs number\n","        print_every (int): defines how many times to print training progress\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    test_error = []\n","    testing_acc = []\n","\n","    # Train model\n","    for epoch in range(0, epochs):\n","        \n","        # Train_step\n","        model, training_acc, optimizer, train_loss = train(train_data, model, criterion, optimizer)\n","        train_losses.append(train_loss)\n","\n","        if epoch % print_every == (print_every - 1):\n","            # Validate_step\n","            with torch.no_grad():\n","                model, valid_loss, error, accuracy = validate(\n","                    validation_data, model, criterion)\n","                valid_losses.append(valid_loss)\n","                test_error.append(error)\n","                testing_acc.append(accuracy)\n","\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Training accuracy: {training_acc:.2f}%\\t'\n","                  f'Test error: {error:.2f}%\\t'\n","                  f'Test accuracy: {accuracy:.2f}%\\t')\n","\n","    # Save results and plot figures\n","    # np.savetxt(os.path.join(RESULTS, \"Test_error.csv\"), test_error, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Train_Losses.csv\"), train_losses, delimiter=\",\")\n","    # np.savetxt(os.path.join(RESULTS, \"Valid_Losses.csv\"), valid_losses, delimiter=\",\")\n","    plot_results(train_losses, valid_losses, test_error, testing_acc)\n","\n","    return model, optimizer, (train_losses, valid_losses, test_error)\n","\n","# plots sc\n","def plot_results_super_convergence(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Cifar10 Aihwkit Super Convergence Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_sc_test_accuracy.png'))\n","    plt.close()\n","\n","def plot_results(train_losses, valid_losses, test_error, test_acc):\n","    \"\"\"Plot results.\n","    Args:\n","        train_losses: training losses as calculated in the training_loop\n","        valid_losses: validation losses as calculated in the training_loop\n","        test_error: test error as calculated in the training_loop\n","    \"\"\"\n","    fig = plt.plot(train_losses, 'r-s', valid_losses, 'b-o')\n","    plt.title('VGG8 Aihwkit Cifar10 Loss')\n","    plt.legend(fig[:2], ['Training Losses', 'Validation Losses'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Loss [A.U.]')\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_losses.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_error, 'r-s')\n","    plt.title('VGG8 Aihwkit Cifar10 Test Error')\n","    plt.legend(fig[:1], ['Test Error'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test Error [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_error.png'))\n","    plt.close()\n","\n","    fig = plt.plot(test_acc, 'r-s')\n","    plt.title('VGG8 Aihwkit Cifar10 Accuracy')\n","    plt.legend(fig[:1], ['Test accuracy'])\n","    plt.xlabel('Epoch number')\n","    plt.ylabel('Test accuracy [%]')\n","    plt.ylim((0, 1e2))\n","    plt.grid(which='both', linestyle='--')\n","    plt.savefig(os.path.join(RESULTS, 'vgg8_cifar10_aihwkit_nsc_test_accuracy.png'))\n","    plt.close()\n","\n","if __name__ == '__main__':\n","\n","    # # create results director for simulation\n","    # create_results_dir()\n","\n","    # set seed\n","    torch.manual_seed(SEED)\n","\n","    # Load datasets.\n","    train_data, validation_data = load_images_cifar10()\n","    # print(f\"train length: {len(train_data)} val length: {len(validation_data)}\")\n","    \n","\n","    # Prepare the model.\n","    model = VGG8(in_channels=3, num_classes=N_CLASSES)\n","    if USE_CUDA:\n","        model.cuda()\n","    \n","    summary(model, (3, 32, 32), BATCH_SIZE)\n","    # exit(0)\n","    # print(model)\n","\n","    # optimizer\n","    optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n","    optimizer.regroup_param_groups(model)\n","\n","    # loss function\n","    criterion = nn.CrossEntropyLoss()\n","\n","\n","    print(f'\\n{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Started Vgg8 Example')\n","    # training loop\n","    model, optimizer, _ = train_val_loop_nsc(model, criterion, optimizer, \n","                                    train_data, validation_data,\n","                                        N_EPOCHS)\n","    print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","      f'Completed Vgg8 Example')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting /content/gdrive/MyDrive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit/examples/vgg8_nsc_aihwkit_cifar10.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gjt_Q34-FzYv"},"source":["### Run code - super convergence"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"rRml_AySGC1U","outputId":"abd3c16a-a0df-4063-8c67-2295a2a0dd5a"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/vgg8_sc_aihwkit_cifar10.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final\n","/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit\n","finished making dir...\n","Normalization data: (tensor([0.4914, 0.4822, 0.4465]),tensor([0.2023, 0.1994, 0.2010]))\n","Files already downloaded and verified\n","Files already downloaded and verified\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","      AnalogConv2d-1         [128, 128, 32, 32]           3,584\n","              ReLU-2         [128, 128, 32, 32]               0\n","      AnalogConv2d-3         [128, 128, 32, 32]         147,584\n","       BatchNorm2d-4         [128, 128, 32, 32]             256\n","              ReLU-5         [128, 128, 32, 32]               0\n","         MaxPool2d-6         [128, 128, 16, 16]               0\n","      AnalogConv2d-7         [128, 256, 16, 16]         295,168\n","              ReLU-8         [128, 256, 16, 16]               0\n","      AnalogConv2d-9         [128, 256, 16, 16]         590,080\n","      BatchNorm2d-10         [128, 256, 16, 16]             512\n","             ReLU-11         [128, 256, 16, 16]               0\n","        MaxPool2d-12           [128, 256, 8, 8]               0\n","     AnalogConv2d-13           [128, 512, 8, 8]       1,180,160\n","             ReLU-14           [128, 512, 8, 8]               0\n","     AnalogConv2d-15           [128, 512, 8, 8]       2,359,808\n","      BatchNorm2d-16           [128, 512, 8, 8]           1,024\n","             ReLU-17           [128, 512, 8, 8]               0\n","        MaxPool2d-18           [128, 512, 4, 4]               0\n","          Flatten-19                [128, 8192]               0\n","     AnalogLinear-20                [128, 1024]       8,389,632\n","             ReLU-21                [128, 1024]               0\n","     AnalogLinear-22                  [128, 10]          10,250\n","       LogSoftmax-23                  [128, 10]               0\n","================================================================\n","Total params: 12,978,058\n","Trainable params: 12,978,058\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.50\n","Forward/backward pass size (MB): 1186.02\n","Params size (MB): 49.51\n","Estimated Total Size (MB): 1237.03\n","----------------------------------------------------------------\n","\n","19:08:42 --- Started Vgg8 Example\n","19:14:24 --- Epoch: 0\tTrain loss: 1.4206\tValid loss: 1.6440\tTraining accuracy: 48.23%\tTest error: 58.77%\tTest accuracy: 41.23%\t\n","19:20:44 --- Epoch: 1\tTrain loss: 1.1402\tValid loss: 1.2462\tTraining accuracy: 59.41%\tTest error: 41.92%\tTest accuracy: 58.08%\t\n","19:27:18 --- Epoch: 2\tTrain loss: 1.0346\tValid loss: 1.0355\tTraining accuracy: 64.72%\tTest error: 35.50%\tTest accuracy: 64.50%\t\n","19:33:58 --- Epoch: 3\tTrain loss: 0.8547\tValid loss: 0.8310\tTraining accuracy: 70.98%\tTest error: 28.23%\tTest accuracy: 71.77%\t\n","19:40:44 --- Epoch: 4\tTrain loss: 0.7140\tValid loss: 1.0023\tTraining accuracy: 75.89%\tTest error: 33.52%\tTest accuracy: 66.48%\t\n","19:47:37 --- Epoch: 5\tTrain loss: 0.6268\tValid loss: 0.8334\tTraining accuracy: 78.92%\tTest error: 26.44%\tTest accuracy: 73.56%\t\n","19:54:37 --- Epoch: 6\tTrain loss: 0.5646\tValid loss: 1.0611\tTraining accuracy: 81.11%\tTest error: 30.66%\tTest accuracy: 69.34%\t\n","20:01:45 --- Epoch: 7\tTrain loss: 0.5184\tValid loss: 0.8362\tTraining accuracy: 82.79%\tTest error: 25.69%\tTest accuracy: 74.31%\t\n","20:08:57 --- Epoch: 8\tTrain loss: 0.4729\tValid loss: 0.8972\tTraining accuracy: 84.47%\tTest error: 27.54%\tTest accuracy: 72.46%\t\n","20:16:12 --- Epoch: 9\tTrain loss: 0.4316\tValid loss: 0.9296\tTraining accuracy: 85.72%\tTest error: 25.47%\tTest accuracy: 74.53%\t\n","20:23:31 --- Epoch: 10\tTrain loss: 0.4015\tValid loss: 0.8043\tTraining accuracy: 86.76%\tTest error: 23.30%\tTest accuracy: 76.70%\t\n","20:30:51 --- Epoch: 11\tTrain loss: 0.3750\tValid loss: 0.9131\tTraining accuracy: 87.62%\tTest error: 22.71%\tTest accuracy: 77.29%\t\n","20:38:16 --- Epoch: 12\tTrain loss: 0.3570\tValid loss: 0.9282\tTraining accuracy: 88.34%\tTest error: 23.52%\tTest accuracy: 76.48%\t\n","20:45:18 --- Epoch: 13\tTrain loss: 0.3133\tValid loss: 0.9605\tTraining accuracy: 89.74%\tTest error: 23.96%\tTest accuracy: 76.04%\t\n","20:51:56 --- Epoch: 14\tTrain loss: 0.2592\tValid loss: 0.7251\tTraining accuracy: 91.61%\tTest error: 18.38%\tTest accuracy: 81.62%\t\n","20:58:15 --- Epoch: 15\tTrain loss: 0.2158\tValid loss: 0.9191\tTraining accuracy: 92.85%\tTest error: 20.74%\tTest accuracy: 79.26%\t\n","21:04:08 --- Epoch: 16\tTrain loss: 0.1640\tValid loss: 0.7966\tTraining accuracy: 94.62%\tTest error: 19.17%\tTest accuracy: 80.83%\t\n","21:09:43 --- Epoch: 17\tTrain loss: 0.1329\tValid loss: 0.8053\tTraining accuracy: 95.64%\tTest error: 17.21%\tTest accuracy: 82.79%\t\n","21:14:54 --- Epoch: 18\tTrain loss: 0.0921\tValid loss: 0.7705\tTraining accuracy: 96.95%\tTest error: 16.25%\tTest accuracy: 83.75%\t\n","21:19:49 --- Epoch: 19\tTrain loss: 0.0667\tValid loss: 0.7915\tTraining accuracy: 97.81%\tTest error: 15.63%\tTest accuracy: 84.37%\t\n","21:24:27 --- Epoch: 20\tTrain loss: 0.0433\tValid loss: 0.8142\tTraining accuracy: 98.59%\tTest error: 14.48%\tTest accuracy: 85.52%\t\n","21:28:52 --- Epoch: 21\tTrain loss: 0.0278\tValid loss: 0.8267\tTraining accuracy: 99.09%\tTest error: 13.77%\tTest accuracy: 86.23%\t\n","21:33:08 --- Epoch: 22\tTrain loss: 0.0155\tValid loss: 0.8173\tTraining accuracy: 99.50%\tTest error: 12.93%\tTest accuracy: 87.07%\t\n","21:37:17 --- Epoch: 23\tTrain loss: 0.0083\tValid loss: 0.8444\tTraining accuracy: 99.73%\tTest error: 12.74%\tTest accuracy: 87.26%\t\n","21:41:22 --- Epoch: 24\tTrain loss: 0.0054\tValid loss: 0.8306\tTraining accuracy: 99.85%\tTest error: 12.44%\tTest accuracy: 87.56%\t\n","21:41:24 --- Completed Vgg8 Example\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZDPxQunTV4sf"},"source":["### Run code - normal training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfLlW3vtVyKr","executionInfo":{"elapsed":876224,"status":"ok","timestamp":1618443207945,"user":{"displayName":"Hung-Yang Chang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN4kq95njvvqCwIFd6x71htqxvxm8299D6yz00=s64","userId":"06619491291973596690"},"user_tz":240},"outputId":"9425e630-6cdc-4506-da52-457c51c126a4"},"source":["%%script bash\n","pwd\n","cd aihwkit_env/aihwkit/\n","pwd\n","export PYTHONPATH=src/\n","PYTHONPATH=src/ python examples/vgg8_nsc_aihwkit_cifar10.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final\n","/content/gdrive/My Drive/Mcgill/Winter2021/ECSE552final/aihwkit_env/aihwkit\n","Normalization data: (tensor([0.4914, 0.4822, 0.4465]),tensor([0.2023, 0.1994, 0.2010]))\n","Files already downloaded and verified\n","Files already downloaded and verified\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","      AnalogConv2d-1         [128, 128, 32, 32]           3,584\n","              ReLU-2         [128, 128, 32, 32]               0\n","      AnalogConv2d-3         [128, 128, 32, 32]         147,584\n","       BatchNorm2d-4         [128, 128, 32, 32]             256\n","              ReLU-5         [128, 128, 32, 32]               0\n","         MaxPool2d-6         [128, 128, 16, 16]               0\n","      AnalogConv2d-7         [128, 256, 16, 16]         295,168\n","              ReLU-8         [128, 256, 16, 16]               0\n","      AnalogConv2d-9         [128, 256, 16, 16]         590,080\n","      BatchNorm2d-10         [128, 256, 16, 16]             512\n","             ReLU-11         [128, 256, 16, 16]               0\n","        MaxPool2d-12           [128, 256, 8, 8]               0\n","     AnalogConv2d-13           [128, 512, 8, 8]       1,180,160\n","             ReLU-14           [128, 512, 8, 8]               0\n","     AnalogConv2d-15           [128, 512, 8, 8]       2,359,808\n","      BatchNorm2d-16           [128, 512, 8, 8]           1,024\n","             ReLU-17           [128, 512, 8, 8]               0\n","        MaxPool2d-18           [128, 512, 4, 4]               0\n","          Flatten-19                [128, 8192]               0\n","     AnalogLinear-20                [128, 1024]       8,389,632\n","             ReLU-21                [128, 1024]               0\n","     AnalogLinear-22                  [128, 10]          10,250\n","       LogSoftmax-23                  [128, 10]               0\n","================================================================\n","Total params: 12,978,058\n","Trainable params: 12,978,058\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.50\n","Forward/backward pass size (MB): 1186.02\n","Params size (MB): 49.51\n","Estimated Total Size (MB): 1237.03\n","----------------------------------------------------------------\n","\n","21:40:13 --- Started Vgg8 Example\n","21:43:04 --- Epoch: 0\tTrain loss: 1.4276\tValid loss: 1.5540\tTraining accuracy: 47.93%\tTest error: 56.19%\tTest accuracy: 43.81%\t\n","21:45:50 --- Epoch: 1\tTrain loss: 1.0385\tValid loss: 1.1112\tTraining accuracy: 62.74%\tTest error: 39.33%\tTest accuracy: 60.67%\t\n","21:48:32 --- Epoch: 2\tTrain loss: 0.8526\tValid loss: 0.9970\tTraining accuracy: 69.87%\tTest error: 35.89%\tTest accuracy: 64.11%\t\n","21:51:09 --- Epoch: 3\tTrain loss: 0.7313\tValid loss: 0.8315\tTraining accuracy: 74.20%\tTest error: 28.80%\tTest accuracy: 71.20%\t\n","21:53:43 --- Epoch: 4\tTrain loss: 0.6415\tValid loss: 0.7408\tTraining accuracy: 77.45%\tTest error: 25.74%\tTest accuracy: 74.26%\t\n","21:56:17 --- Epoch: 5\tTrain loss: 0.5677\tValid loss: 0.7169\tTraining accuracy: 80.05%\tTest error: 24.39%\tTest accuracy: 75.61%\t\n","21:58:49 --- Epoch: 6\tTrain loss: 0.5099\tValid loss: 0.8254\tTraining accuracy: 82.16%\tTest error: 28.31%\tTest accuracy: 71.69%\t\n","22:01:21 --- Epoch: 7\tTrain loss: 0.4593\tValid loss: 0.6876\tTraining accuracy: 83.97%\tTest error: 23.93%\tTest accuracy: 76.07%\t\n","22:03:51 --- Epoch: 8\tTrain loss: 0.4056\tValid loss: 0.7770\tTraining accuracy: 86.00%\tTest error: 26.26%\tTest accuracy: 73.74%\t\n","22:06:21 --- Epoch: 9\tTrain loss: 0.3649\tValid loss: 0.6847\tTraining accuracy: 87.37%\tTest error: 22.78%\tTest accuracy: 77.22%\t\n","22:08:48 --- Epoch: 10\tTrain loss: 0.3223\tValid loss: 0.9179\tTraining accuracy: 88.98%\tTest error: 28.19%\tTest accuracy: 71.81%\t\n","22:11:14 --- Epoch: 11\tTrain loss: 0.2852\tValid loss: 0.7197\tTraining accuracy: 90.36%\tTest error: 22.53%\tTest accuracy: 77.47%\t\n","22:13:39 --- Epoch: 12\tTrain loss: 0.2478\tValid loss: 0.5780\tTraining accuracy: 91.75%\tTest error: 19.09%\tTest accuracy: 80.91%\t\n","22:16:04 --- Epoch: 13\tTrain loss: 0.2211\tValid loss: 0.6452\tTraining accuracy: 92.70%\tTest error: 20.76%\tTest accuracy: 79.24%\t\n","22:18:28 --- Epoch: 14\tTrain loss: 0.1870\tValid loss: 0.6609\tTraining accuracy: 94.14%\tTest error: 20.60%\tTest accuracy: 79.40%\t\n","22:20:51 --- Epoch: 15\tTrain loss: 0.1505\tValid loss: 0.9176\tTraining accuracy: 95.45%\tTest error: 26.34%\tTest accuracy: 73.66%\t\n","22:23:13 --- Epoch: 16\tTrain loss: 0.1311\tValid loss: 0.6035\tTraining accuracy: 96.16%\tTest error: 19.24%\tTest accuracy: 80.76%\t\n","22:25:33 --- Epoch: 17\tTrain loss: 0.1054\tValid loss: 0.5897\tTraining accuracy: 97.11%\tTest error: 17.63%\tTest accuracy: 82.37%\t\n","22:27:51 --- Epoch: 18\tTrain loss: 0.0799\tValid loss: 0.6278\tTraining accuracy: 98.10%\tTest error: 18.14%\tTest accuracy: 81.86%\t\n","22:30:05 --- Epoch: 19\tTrain loss: 0.0633\tValid loss: 0.7328\tTraining accuracy: 98.61%\tTest error: 20.50%\tTest accuracy: 79.50%\t\n","22:32:17 --- Epoch: 20\tTrain loss: 0.0460\tValid loss: 0.6458\tTraining accuracy: 99.30%\tTest error: 18.63%\tTest accuracy: 81.37%\t\n","22:34:28 --- Epoch: 21\tTrain loss: 0.0354\tValid loss: 0.5655\tTraining accuracy: 99.58%\tTest error: 16.41%\tTest accuracy: 83.59%\t\n","22:36:37 --- Epoch: 22\tTrain loss: 0.0267\tValid loss: 0.5929\tTraining accuracy: 99.80%\tTest error: 16.89%\tTest accuracy: 83.11%\t\n","22:38:46 --- Epoch: 23\tTrain loss: 0.0217\tValid loss: 0.5565\tTraining accuracy: 99.87%\tTest error: 16.00%\tTest accuracy: 84.00%\t\n","22:40:55 --- Epoch: 24\tTrain loss: 0.0175\tValid loss: 0.5571\tTraining accuracy: 99.93%\tTest error: 15.88%\tTest accuracy: 84.12%\t\n","22:43:03 --- Epoch: 25\tTrain loss: 0.0150\tValid loss: 0.5735\tTraining accuracy: 99.96%\tTest error: 16.04%\tTest accuracy: 83.96%\t\n","22:45:10 --- Epoch: 26\tTrain loss: 0.0129\tValid loss: 0.5688\tTraining accuracy: 99.96%\tTest error: 15.75%\tTest accuracy: 84.25%\t\n","22:47:16 --- Epoch: 27\tTrain loss: 0.0110\tValid loss: 0.5758\tTraining accuracy: 99.98%\tTest error: 15.76%\tTest accuracy: 84.24%\t\n","22:49:23 --- Epoch: 28\tTrain loss: 0.0097\tValid loss: 0.5793\tTraining accuracy: 99.99%\tTest error: 15.56%\tTest accuracy: 84.44%\t\n","22:51:29 --- Epoch: 29\tTrain loss: 0.0086\tValid loss: 0.5846\tTraining accuracy: 99.99%\tTest error: 15.76%\tTest accuracy: 84.24%\t\n","22:53:35 --- Epoch: 30\tTrain loss: 0.0079\tValid loss: 0.5951\tTraining accuracy: 99.99%\tTest error: 15.67%\tTest accuracy: 84.33%\t\n","22:55:41 --- Epoch: 31\tTrain loss: 0.0071\tValid loss: 0.5837\tTraining accuracy: 100.00%\tTest error: 15.49%\tTest accuracy: 84.51%\t\n","22:57:47 --- Epoch: 32\tTrain loss: 0.0070\tValid loss: 0.5884\tTraining accuracy: 100.00%\tTest error: 15.56%\tTest accuracy: 84.44%\t\n","22:59:53 --- Epoch: 33\tTrain loss: 0.0062\tValid loss: 0.5913\tTraining accuracy: 100.00%\tTest error: 15.61%\tTest accuracy: 84.39%\t\n","23:01:59 --- Epoch: 34\tTrain loss: 0.0058\tValid loss: 0.5834\tTraining accuracy: 99.99%\tTest error: 15.25%\tTest accuracy: 84.75%\t\n","23:04:04 --- Epoch: 35\tTrain loss: 0.0054\tValid loss: 0.5898\tTraining accuracy: 100.00%\tTest error: 15.31%\tTest accuracy: 84.69%\t\n","23:06:10 --- Epoch: 36\tTrain loss: 0.0049\tValid loss: 0.6084\tTraining accuracy: 100.00%\tTest error: 15.43%\tTest accuracy: 84.57%\t\n","23:08:16 --- Epoch: 37\tTrain loss: 0.0048\tValid loss: 0.6052\tTraining accuracy: 100.00%\tTest error: 15.42%\tTest accuracy: 84.58%\t\n","23:10:22 --- Epoch: 38\tTrain loss: 0.0045\tValid loss: 0.5973\tTraining accuracy: 100.00%\tTest error: 15.30%\tTest accuracy: 84.70%\t\n","23:12:27 --- Epoch: 39\tTrain loss: 0.0042\tValid loss: 0.6027\tTraining accuracy: 100.00%\tTest error: 15.02%\tTest accuracy: 84.98%\t\n","23:14:33 --- Epoch: 40\tTrain loss: 0.0041\tValid loss: 0.5939\tTraining accuracy: 100.00%\tTest error: 15.22%\tTest accuracy: 84.78%\t\n","23:16:38 --- Epoch: 41\tTrain loss: 0.0037\tValid loss: 0.6094\tTraining accuracy: 100.00%\tTest error: 15.05%\tTest accuracy: 84.95%\t\n","23:18:44 --- Epoch: 42\tTrain loss: 0.0036\tValid loss: 0.6111\tTraining accuracy: 100.00%\tTest error: 15.39%\tTest accuracy: 84.61%\t\n","23:20:50 --- Epoch: 43\tTrain loss: 0.0034\tValid loss: 0.6096\tTraining accuracy: 100.00%\tTest error: 15.05%\tTest accuracy: 84.95%\t\n","23:22:56 --- Epoch: 44\tTrain loss: 0.0033\tValid loss: 0.6171\tTraining accuracy: 100.00%\tTest error: 15.17%\tTest accuracy: 84.83%\t\n","23:25:02 --- Epoch: 45\tTrain loss: 0.0031\tValid loss: 0.6099\tTraining accuracy: 100.00%\tTest error: 15.22%\tTest accuracy: 84.78%\t\n","23:27:08 --- Epoch: 46\tTrain loss: 0.0030\tValid loss: 0.6239\tTraining accuracy: 100.00%\tTest error: 15.36%\tTest accuracy: 84.64%\t\n","23:29:14 --- Epoch: 47\tTrain loss: 0.0029\tValid loss: 0.6135\tTraining accuracy: 100.00%\tTest error: 14.88%\tTest accuracy: 85.12%\t\n","23:31:20 --- Epoch: 48\tTrain loss: 0.0028\tValid loss: 0.6139\tTraining accuracy: 100.00%\tTest error: 15.36%\tTest accuracy: 84.64%\t\n","23:33:26 --- Epoch: 49\tTrain loss: 0.0026\tValid loss: 0.6209\tTraining accuracy: 100.00%\tTest error: 15.20%\tTest accuracy: 84.80%\t\n","23:33:26 --- Completed Vgg8 Example\n"],"name":"stdout"}]}]}